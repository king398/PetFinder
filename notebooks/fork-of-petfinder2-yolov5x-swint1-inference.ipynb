{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import sys\nsys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom tqdm.notebook import tqdm\nimport os, gc\nimport random\nfrom PIL import Image\nimport tifffile as tiff\nimport cv2\nimport zipfile\nimport collections\nfrom PIL import Image\nfrom sklearn import preprocessing\nfrom random import randint\nfrom glob import glob\nimport shutil\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport torchvision\nfrom torchvision import transforms\nimport albumentations as A\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed = 2020\nseed_everything(seed)\nsz = 224\nNFOLDS = 5\n\n#ImageNet\nmean = np.array([[[0.485, 0.456, 0.406]]])\nstd = np.array([[[0.229, 0.224, 0.225]]])",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2021-12-13T10:52:59.784895Z",
     "iopub.execute_input": "2021-12-13T10:52:59.785363Z",
     "iopub.status.idle": "2021-12-13T10:53:08.800201Z",
     "shell.execute_reply.started": "2021-12-13T10:52:59.785237Z",
     "shell.execute_reply": "2021-12-13T10:53:08.799177Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\ntest_ids = test_df.Id.to_list()\ntest_dir = \"/kaggle/input/petfinder-pawpularity-score/test/\"\nshutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\nos.chdir('/kaggle/working/yolov5')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:08.802914Z",
     "iopub.execute_input": "2021-12-13T10:53:08.803154Z",
     "iopub.status.idle": "2021-12-13T10:53:09.49248Z",
     "shell.execute_reply.started": "2021-12-13T10:53:08.803127Z",
     "shell.execute_reply": "2021-12-13T10:53:09.491098Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python detect.py\\\n--weights /kaggle/input/ultralyticsyolov5aweights/yolov5x.pt\\\n--class 15 16\\\n--img 512\\\n--conf 0.3\\\n--iou 0.5\\\n--source $test_dir\\\n--name inference\\\n--save-txt --save-conf --exist-ok",
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:09.494143Z",
     "iopub.execute_input": "2021-12-13T10:53:09.494443Z",
     "iopub.status.idle": "2021-12-13T10:53:24.580371Z",
     "shell.execute_reply.started": "2021-12-13T10:53:09.494405Z",
     "shell.execute_reply": "2021-12-13T10:53:24.579048Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "os.chdir('/kaggle/working')\nsave_dir = f'/kaggle/working/crop_images/'\nos.makedirs(save_dir, exist_ok=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:24.582343Z",
     "iopub.execute_input": "2021-12-13T10:53:24.58293Z",
     "iopub.status.idle": "2021-12-13T10:53:24.589952Z",
     "shell.execute_reply.started": "2021-12-13T10:53:24.582821Z",
     "shell.execute_reply": "2021-12-13T10:53:24.588312Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for n, image_id in tqdm(enumerate(test_ids)):\n    xmin = []\n    ymin = []\n    xmax = []\n    ymax = []\n    orig_image = cv2.imread(f'/kaggle/input/petfinder-pawpularity-score/test/{image_id}.jpg')\n    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    height = orig_image.shape[0]\n    width = orig_image.shape[1]\n    try:\n        file_path = f'/kaggle/working/yolov5/runs/detect/inference/labels/{image_id}.txt'\n        f = open(file_path, 'r')\n        data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n        data = data[:, [0, 5, 1, 2, 3, 4]]\n        for i, d in enumerate(data):\n            xmin.append((d[2]-d[4]/2)*width)\n            ymin.append((d[3]-d[5]/2)*height)\n            xmax.append((d[2]+d[4]/2)*width)\n            ymax.append((d[3]+d[5]/2)*height)\n        all_xmin = int(min(xmin))\n        all_ymin = int(min(ymin))\n        all_xmax = int(max(xmax))\n        all_ymax = int(max(ymax))\n        all_width_half = (all_xmax - all_xmin) // 2\n        all_height_half = (all_ymax - all_ymin) // 2\n        r = np.maximum(all_width_half, all_height_half)\n        all_xc = (all_xmin + all_xmax) // 2\n        all_yc = (all_ymin + all_ymax) // 2\n        final_xmin = np.maximum(all_xc-r, 0)\n        final_ymin = np.maximum(all_yc-r, 0)\n        final_xmax = np.minimum(all_xc+r, width)\n        final_ymax = np.minimum(all_yc+r, height)\n        crop_img = orig_image[final_ymin:final_ymax, final_xmin:final_xmax, :]\n        crop_img = cv2.resize(crop_img, (sz, sz)).astype(np.uint8)\n        np.save(save_dir + f'{image_id}', crop_img)\n    except:\n        orig_image = cv2.resize(orig_image, (sz, sz)).astype(np.uint8)\n        np.save(save_dir + f'{image_id}', orig_image)",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:24.59262Z",
     "iopub.execute_input": "2021-12-13T10:53:24.593379Z",
     "iopub.status.idle": "2021-12-13T10:53:24.689624Z",
     "shell.execute_reply.started": "2021-12-13T10:53:24.593337Z",
     "shell.execute_reply": "2021-12-13T10:53:24.688747Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!rm -r /kaggle/working/yolov5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:24.69124Z",
     "iopub.execute_input": "2021-12-13T10:53:24.691732Z",
     "iopub.status.idle": "2021-12-13T10:53:25.445152Z",
     "shell.execute_reply.started": "2021-12-13T10:53:24.691696Z",
     "shell.execute_reply": "2021-12-13T10:53:25.443784Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(len(os.listdir(\"./crop_images\")))\nprint(os.listdir(\"./crop_images\")[:1])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:25.447996Z",
     "iopub.execute_input": "2021-12-13T10:53:25.448251Z",
     "iopub.status.idle": "2021-12-13T10:53:25.457125Z",
     "shell.execute_reply.started": "2021-12-13T10:53:25.448218Z",
     "shell.execute_reply": "2021-12-13T10:53:25.456011Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import sys\n\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n# Asthetics\nimport warnings\nimport sklearn.exceptions\n\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport random\nimport cv2\n\npd.set_option('display.max_columns', None)\n\n# Image Aug\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Random Seed Initialize\nRANDOM_SEED = 2021\n\n\ndef seed_everything(seed=RANDOM_SEED):\n\tos.environ['PYTHONHASHSEED'] = str(seed)\n\tnp.random.seed(seed)\n\trandom.seed(seed)\n\ttorch.manual_seed(seed)\n\ttorch.cuda.manual_seed(seed)\n\ttorch.backends.cudnn.deterministic = True\n\ttorch.backends.cudnn.benchmark = True\n\n\nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n\tdevice = torch.device('cuda')\nelse:\n\tdevice = torch.device('cpu')\n\nprint(f'Using device: {device}')\n\ncsv_dir = '../input/petfinder-pawpularity-score'\ntest_dir = './crop_images'\nmodels_dir = '../input/swin-transformenrs-pet-net'\n\ntest_file_path = os.path.join(csv_dir, 'test.csv')\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\nprint(f'Test file: {test_file_path}')\nprint(f'Models path: {models_dir}')\n\ntest_df = pd.read_csv(test_file_path)\nsample_df = pd.read_csv(sample_sub_file_path)\n\n\ndef return_filpath(name, folder):\n\tpath = os.path.join(folder, f'{name}.npy')\n\treturn path\n\n\ntest_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))\ntest_df.head()\n\ndf = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\nprint('Train shape:', df.shape)\ndf.head()\n\nparams = {\n\t'model': 'swin_large_patch4_window12_384',\n\t'model2': 'swin_large_patch4_window7_224',\n\t'dense_features': ['Subject Focus', 'Eyes', 'Face', 'Near',\n\t                   'Action', 'Accessory', 'Group', 'Collage',\n\t                   'Human', 'Occlusion', 'Info', 'Blur'],\n\t'pretrained': False,\n\t'inp_channels': 3,\n\t'im_size': 384,\n\t'device': device,\n\t'batch_size': 8,\n\t'num_workers': 0,\n\t'out_features': 1,\n\t'debug': False\n}\nif params['debug']:\n\ttest_df = test_df.sample(frac=0.1)\n\n\ndef get_test_transforms(DIM=params['im_size']):\n\treturn albumentations.Compose(\n\t\t[\n\t\t\talbumentations.Resize(DIM, DIM),\n\t\t\talbumentations.Normalize(\n\t\t\t\tmean=[0.485, 0.456, 0.406],\n\t\t\t\tstd=[0.229, 0.224, 0.225],\n\t\t\t),\n\t\t\tToTensorV2(p=1.0)\n\t\t]\n\t)\n\n\ndef get_test_Flip_transforms(DIM=params['im_size']):\n\treturn albumentations.Compose(\n\t\t[albumentations.HorizontalFlip(p=0.5),\n\n         albumentations.VerticalFlip(p=0.5),\n\t\t albumentations.Resize(DIM, DIM),\n\t\t albumentations.Normalize(\n\t\t\t mean=[0.485, 0.456, 0.406],\n\t\t\t std=[0.229, 0.224, 0.225],\n\t\t ),\n\t\t ToTensorV2(p=1.0)\n\t\t ]\n\t)\n\n\ndef get_test_Shift_Scale_transforms(DIM=params['im_size']):\n\treturn albumentations.Compose(\n\t\t[albumentations.ShiftScaleRotate(\n\t\t\tshift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n\t\t),\n\n\t\t\talbumentations.Resize(DIM, DIM),\n\t\t\talbumentations.Normalize(\n\t\t\t\tmean=[0.485, 0.456, 0.406],\n\t\t\t\tstd=[0.229, 0.224, 0.225],\n\t\t\t),\n\t\t\tToTensorV2(p=1.0)\n\t\t]\n\t)\n\n\ndef get_test_Rotate_transforms(DIM=params['im_size']):\n\treturn albumentations.Compose(\n\t\t[albumentations.Rotate(limit=180, p=0.7),\n\t\t albumentations.Resize(DIM, DIM),\n\t\t albumentations.Normalize(\n\t\t\t mean=[0.485, 0.456, 0.406],\n\t\t\t std=[0.229, 0.224, 0.225],\n\t\t ),\n\t\t ToTensorV2(p=1.0)\n\t\t ]\n\t)\n\n\nclass CuteDataset(Dataset):\n\tdef __init__(self, images_filepaths, dense_features, targets, transform=None, transform_flip=None,\n\t             transform_shiftscale=None, transform_rotate=None):\n\t\tself.images_filepaths = images_filepaths\n\t\tself.dense_features = dense_features\n\t\tself.targets = targets\n\t\tself.transform = transform\n\t\tself.transform_flip = transform_flip\n\t\tself.transform_shiftscale = transform_shiftscale\n\t\tself.transform_rotate = transform_rotate\n\n\tdef __len__(self):\n\t\treturn len(self.images_filepaths)\n\n\tdef __getitem__(self, idx):\n\t\timage_filepath = self.images_filepaths[idx]\n\t\timage = np.load(image_filepath).astype(np.float32)\n\n\t\tif self.transform is not None:\n\t\t\timage1 = self.transform(image=image)['image']\n\t\tif self.transform_flip is not None:\n\t\t\timage2 = self.transform_flip(image=image)['image']\n\t\tif self.transform_shiftscale is not None:\n\t\t\timage3 = self.transform_shiftscale(image=image)['image']\n\t\tif self.transform_shiftscale is not None:\n\t\t\timage4 = self.transform_rotate(image=image)['image']\n\n\t\tdense = self.dense_features[idx, :]\n\t\tlabel = torch.tensor(self.targets[idx]).float()\n\t\treturn image1, image2, image3, image4, dense, label\n\n\nclass PetNet(nn.Module):\n\tdef __init__(self, model_name=params['model'], out_features=params['out_features'],\n\t             inp_channels=params['inp_channels'],\n\t             pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n\t\tsuper().__init__()\n\t\tself.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n\t\tn_features = self.model.head.in_features\n\t\tself.model.head = nn.Linear(n_features, 128)\n\t\tself.fc = nn.Sequential(\n\t\t\tnn.Linear(128 + num_dense, 64),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Linear(64, out_features)\n\t\t)\n\t\tself.dropout = nn.Dropout(0.2)\n\n\tdef forward(self, image, dense):\n\t\tembeddings = self.model(image)\n\t\tx = self.dropout(embeddings)\n\t\tx = torch.cat([x, dense], dim=1)\n\t\toutput = self.fc(x)\n\t\treturn output\n\n\nclass PetNet2(nn.Module):\n\tdef __init__(self, model_name=params['model2'], out_features=params['out_features'],\n\t             inp_channels=params['inp_channels'],\n\t             pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n\t\tsuper().__init__()\n\t\tself.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n\t\tn_features = self.model.head.in_features\n\t\tself.model.head = nn.Linear(n_features, 128)\n\t\tself.fc = nn.Sequential(\n\t\t\tnn.Linear(128, 64),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Linear(64, out_features)\n\t\t)\n\t\tself.dropout = nn.Dropout(0.2)\n\n\tdef forward(self, image, dense):\n\t\tembeddings = self.model(image)\n\t\tx = self.dropout(embeddings)\n\t\tx = torch.cat([x], dim=1)\n\t\toutput = self.fc(x)\n\t\treturn output\n\n\n\n\npredicted_labels = None\nfor model_name in glob.glob(\"../input/imagenet-1k-swin-large-384/SwinLarge384Withcrop\" + '/*.pth'):\n\tmodel = PetNet()\n\tmodel.load_state_dict(torch.load(model_name))\n\tmodel = model.to(params['device'])\n\tmodel.eval()\n\n\ttest_dataset = CuteDataset(\n\t\timages_filepaths=test_df['image_path'].values,\n\t\tdense_features=test_df[params['dense_features']].values,\n\t\ttargets=sample_df['Pawpularity'].values,\n\t\ttransform=get_test_transforms(),\n\t\ttransform_flip=get_test_Flip_transforms(),\n\t\ttransform_shiftscale=get_test_Shift_Scale_transforms(),\n\t\ttransform_rotate=get_test_Rotate_transforms()\n\n\t)\n\ttest_loader = DataLoader(\n\t\ttest_dataset, batch_size=params['batch_size'],\n\t\tshuffle=False, num_workers=params['num_workers'],\n\t\tpin_memory=True\n\t)\n\n\ttemp_preds = None\n\twith torch.no_grad():\n\t\tfor (images, images_flip, images_shift, images_rotate, dense, target) in tqdm(test_loader,\n\t\t                                                                              desc=f'Predicting. '):\n\t\t\timages = images.to(params['device'], non_blocking=True)\n\t\t\timages_flip = images_flip.to(params['device'], non_blocking=True)\n\t\t\timages_shift = images_shift.to(params['device'], non_blocking=True)\n\t\t\timages_rotate = images_rotate.to(params['device'], non_blocking=True)\n\t\t\tdense = dense.to(params['device'], non_blocking=True)\n\t\t\tpredictions = torch.sigmoid(model(images, dense)).to('cpu').numpy() * 100\n\t\t\tpredictions += torch.sigmoid(model(images_flip, dense)).to('cpu').numpy() * 100\n\t\t\tpredictions += torch.sigmoid(model(images_shift, dense)).to('cpu').numpy() * 100\n\t\t\tpredictions += torch.sigmoid(model(images_rotate, dense)).to('cpu').numpy() * 100\n\n\t\t\tpredictions = predictions / 4;print(predictions)\n\t\t\tif temp_preds is None:\n\t\t\t\ttemp_preds = predictions\n\t\t\telse:\n\t\t\t\ttemp_preds = np.vstack((temp_preds, predictions))\n\n\tif predicted_labels is None:\n\t\tpredicted_labels = temp_preds\n\telse:\n\t\tpredicted_labels += temp_preds\n\npredicted_labels /= (len(glob.glob(\"../input/imagenet-1k-swin-large-384/SwinLarge384Withcrop\" + '/*.pth')))\n\ntest_dataset = CuteDataset(\n\timages_filepaths=test_df['image_path'].values,\n\tdense_features=test_df[params['dense_features']].values,\n\ttargets=sample_df['Pawpularity'].values,\n\ttransform=get_test_transforms(224),\n\ttransform_flip=get_test_Flip_transforms(224),\n\ttransform_shiftscale=get_test_Shift_Scale_transforms(224),\n\ttransform_rotate=get_test_Rotate_transforms(224)\n)\n\ntest_loader = DataLoader(\n\ttest_dataset, batch_size=params['batch_size'],\n\tshuffle=False, num_workers=params['num_workers'],\n\tpin_memory=True\n)\n\npredicted_labels2 = None\nfor model_name in glob.glob(\"../input/imagenet-1k-swin/SwinLargw224Nonmixp\" + '/*.pth'):\n\tmodel = PetNet2()\n\tmodel.load_state_dict(torch.load(model_name))\n\tmodel = model.to(params['device'])\n\tmodel.eval()\n\n\ttemp_preds = None\n\twith torch.no_grad():\n\t\tfor (images, images_flip, images_shift, images_rotate, dense, target) in tqdm(test_loader,\n\t\t                                                                              desc=f'Predicting. '):\n\t\t\timages = images.to(params['device'], non_blocking=True)\n\t\t\timages_flip = images_flip.to(params['device'], non_blocking=True)\n\t\t\timages_shift = images_shift.to(params['device'], non_blocking=True)\n\t\t\timages_rotate = images_rotate.to(params['device'], non_blocking=True)\n\t\t\tdense = dense.to(params['device'], non_blocking=True)\n\t\t\tpredictions = torch.sigmoid(model(images, dense)).to('cpu').numpy() * 100\n\t\t\tpredictions += torch.sigmoid(model(images_flip, dense)).to('cpu').numpy() * 100\n\t\t\tpredictions += torch.sigmoid(model(images_shift, dense)).to('cpu').numpy() * 100\n\t\t\tpredictions += torch.sigmoid(model(images_rotate, dense)).to('cpu').numpy() * 100\n\n\t\t\tpredictions = predictions / 4;print(predictions)\n\t\t\tif temp_preds is None:\n\t\t\t\ttemp_preds = predictions\n\t\t\telse:\n\t\t\t\ttemp_preds = np.vstack((temp_preds, predictions))\n\n\tif predicted_labels2 is None:\n\t\tpredicted_labels2 = temp_preds\n\telse:\n\t\tpredicted_labels2 += temp_preds\n\n#     del model\n\npredicted_labels2 /= (len(glob.glob(\"../input/imagenet-1k-swin/SwinLargw224Nonmixp\" + '/*.pth')))\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:25.459709Z",
     "iopub.execute_input": "2021-12-13T10:53:25.460529Z",
     "iopub.status.idle": "2021-12-13T10:57:20.666147Z",
     "shell.execute_reply.started": "2021-12-13T10:53:25.460487Z",
     "shell.execute_reply": "2021-12-13T10:57:20.664817Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport random\nimport cv2\npd.set_option('display.max_columns', None)\n\n# Image Aug\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Random Seed Initialize\nRANDOM_SEED = 2021\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')\n\n\ncsv_dir = '../input/petfinder-pawpularity-score'\ntest_dir = '../input/petfinder-pawpularity-score/test'\nmodels_dir = '../input/swin-transformenrs-pet-net'\n\ntest_file_path = os.path.join(csv_dir, 'test.csv')\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\nprint(f'Test file: {test_file_path}')\nprint(f'Models path: {models_dir}')\n\ntest_df = pd.read_csv(test_file_path)\nsample_df = pd.read_csv(sample_sub_file_path)\n\ndef return_filpath(name, folder):\n    path = os.path.join(folder, f'{name}.jpg')\n    return path\ntest_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))\ntest_df.head()\n\ndf = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\nprint('Train shape:', df.shape )\ndf.head()\n\nparams = {\n    'model': 'swin_large_patch4_window12_384_in22k',\n    'model2' : 'vit_large_patch16_224_in21k',\n    'dense_features': ['Subject Focus', 'Eyes', 'Face', 'Near',\n                       'Action', 'Accessory', 'Group', 'Collage',\n                       'Human', 'Occlusion', 'Info', 'Blur'],\n    'pretrained': False,\n    'inp_channels': 3,\n    'im_size': 384,\n    'device': device,\n    'batch_size': 16,\n    'num_workers' : 0,\n    'out_features': 1,\n    'debug': False\n}\n\nif params['debug']:\n    test_df = test_df.sample(frac=0.1)\n\n    \ndef get_test_transforms(DIM = params['im_size']):\n    return albumentations.Compose(\n        [albumentations.HorizontalFlip(p=0.5),\n\n         albumentations.VerticalFlip(p=0.5),\n          albumentations.Resize(DIM,DIM),\n          albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n          ),\n          ToTensorV2(p=1.0)\n        ]\n    )\n\n\nclass CuteDataset(Dataset):\n    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        dense = self.dense_features[idx, :]\n        label = torch.tensor(self.targets[idx]).float()\n        return image, dense, label\n\nclass PetNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'],\n                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, 128)\n        self.fc = nn.Sequential(\n            nn.Linear(128 + num_dense, 64),\n            nn.ReLU(),\n            nn.Linear(64, out_features)\n        )\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, image, dense):\n        embeddings = self.model(image)\n        x = self.dropout(embeddings)\n        x = torch.cat([x, dense], dim=1)\n        output = self.fc(x)\n        return output\n\ntest_dataset = CuteDataset(\n    images_filepaths = test_df['image_path'].values,\n    dense_features = test_df[params['dense_features']].values,\n    targets = sample_df['Pawpularity'].values,\n    transform = get_test_transforms()\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=params['batch_size'],\n    shuffle=False, num_workers=params['num_workers'],\n    pin_memory=True\n)\n\npredicted_labels1 = None\nfor model_name in glob.glob(\"../input/swin-transformenrs-pet-net\" + '/*.pth'):\n    model = PetNet()\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(params['device'])\n    model.eval()\n\n    test_dataset = CuteDataset(\n        images_filepaths = test_df['image_path'].values,\n        dense_features = test_df[params['dense_features']].values,\n        targets = sample_df['Pawpularity'].values,\n        transform = get_test_transforms()\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for (images, dense, target) in tqdm(test_loader, desc=f'Predicting. '):\n            images = images.to(params['device'], non_blocking=True)\n            dense = dense.to(params['device'], non_blocking=True)\n            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n\n    if predicted_labels1 is None:\n        predicted_labels1 = temp_preds\n    else:\n        predicted_labels1 += temp_preds\n        \npredicted_labels1 /= (len(glob.glob(\"../input/swin-transformenrs-pet-net\" + '/*.pth')))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "predicted_labels = predicted_labels * 1/3 + predicted_labels2 * 1/3 + predicted_labels1 * 1/3",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(predicted_labels2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:58:20.1881Z",
     "iopub.execute_input": "2021-12-13T10:58:20.188993Z",
     "iopub.status.idle": "2021-12-13T10:58:20.195873Z",
     "shell.execute_reply.started": "2021-12-13T10:58:20.188957Z",
     "shell.execute_reply": "2021-12-13T10:58:20.193844Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!rm -r /kaggle/working/crop_images",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:57:20.667535Z",
     "iopub.status.idle": "2021-12-13T10:57:20.668857Z",
     "shell.execute_reply.started": "2021-12-13T10:57:20.668503Z",
     "shell.execute_reply": "2021-12-13T10:57:20.668561Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sub_df = pd.DataFrame()\nsub_df['Id'] = test_df['Id']\nsub_df['Pawpularity'] = predicted_labels\nsub_df.to_csv('submission.csv', index=False)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:57:20.670113Z",
     "iopub.status.idle": "2021-12-13T10:57:20.672193Z",
     "shell.execute_reply.started": "2021-12-13T10:57:20.671858Z",
     "shell.execute_reply": "2021-12-13T10:57:20.671891Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}