{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os, gc\n",
    "import random\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import zipfile\n",
    "import collections\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from random import randint\n",
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 2020\n",
    "seed_everything(seed)\n",
    "sz = 224\n",
    "NFOLDS = 5\n",
    "\n",
    "#ImageNet\n",
    "mean = np.array([[[0.485, 0.456, 0.406]]])\n",
    "std = np.array([[[0.229, 0.224, 0.225]]])"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2021-12-13T10:52:59.784895Z",
     "iopub.execute_input": "2021-12-13T10:52:59.785363Z",
     "iopub.status.idle": "2021-12-13T10:53:08.800201Z",
     "shell.execute_reply.started": "2021-12-13T10:52:59.785237Z",
     "shell.execute_reply": "2021-12-13T10:53:08.799177Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\n",
    "test_ids = test_df.Id.to_list()\n",
    "test_dir = \"/kaggle/input/petfinder-pawpularity-score/test/\"\n",
    "shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\n",
    "os.chdir('/kaggle/working/yolov5')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:08.802914Z",
     "iopub.execute_input": "2021-12-13T10:53:08.803154Z",
     "iopub.status.idle": "2021-12-13T10:53:09.49248Z",
     "shell.execute_reply.started": "2021-12-13T10:53:08.803127Z",
     "shell.execute_reply": "2021-12-13T10:53:09.491098Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python detect.py\\\n",
    "--weights /kaggle/input/ultralyticsyolov5aweights/yolov5x.pt\\\n",
    "--class 15 16\\\n",
    "--img 512\\\n",
    "--conf 0.3\\\n",
    "--iou 0.5\\\n",
    "--source $test_dir\\\n",
    "--name inference\\\n",
    "--save-txt --save-conf --exist-ok"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:09.494143Z",
     "iopub.execute_input": "2021-12-13T10:53:09.494443Z",
     "iopub.status.idle": "2021-12-13T10:53:24.580371Z",
     "shell.execute_reply.started": "2021-12-13T10:53:09.494405Z",
     "shell.execute_reply": "2021-12-13T10:53:24.579048Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "os.chdir('/kaggle/working')\n",
    "save_dir = f'/kaggle/working/crop_images/'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:24.582343Z",
     "iopub.execute_input": "2021-12-13T10:53:24.58293Z",
     "iopub.status.idle": "2021-12-13T10:53:24.589952Z",
     "shell.execute_reply.started": "2021-12-13T10:53:24.582821Z",
     "shell.execute_reply": "2021-12-13T10:53:24.588312Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for n, image_id in tqdm(enumerate(test_ids)):\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    orig_image = cv2.imread(f'/kaggle/input/petfinder-pawpularity-score/test/{image_id}.jpg')\n",
    "    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "    height = orig_image.shape[0]\n",
    "    width = orig_image.shape[1]\n",
    "    try:\n",
    "        file_path = f'/kaggle/working/yolov5/runs/detect/inference/labels/{image_id}.txt'\n",
    "        f = open(file_path, 'r')\n",
    "        data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n",
    "        data = data[:, [0, 5, 1, 2, 3, 4]]\n",
    "        for i, d in enumerate(data):\n",
    "            xmin.append((d[2]-d[4]/2)*width)\n",
    "            ymin.append((d[3]-d[5]/2)*height)\n",
    "            xmax.append((d[2]+d[4]/2)*width)\n",
    "            ymax.append((d[3]+d[5]/2)*height)\n",
    "        all_xmin = int(min(xmin))\n",
    "        all_ymin = int(min(ymin))\n",
    "        all_xmax = int(max(xmax))\n",
    "        all_ymax = int(max(ymax))\n",
    "        all_width_half = (all_xmax - all_xmin) // 2\n",
    "        all_height_half = (all_ymax - all_ymin) // 2\n",
    "        r = np.maximum(all_width_half, all_height_half)\n",
    "        all_xc = (all_xmin + all_xmax) // 2\n",
    "        all_yc = (all_ymin + all_ymax) // 2\n",
    "        final_xmin = np.maximum(all_xc-r, 0)\n",
    "        final_ymin = np.maximum(all_yc-r, 0)\n",
    "        final_xmax = np.minimum(all_xc+r, width)\n",
    "        final_ymax = np.minimum(all_yc+r, height)\n",
    "        crop_img = orig_image[final_ymin:final_ymax, final_xmin:final_xmax, :]\n",
    "        crop_img = cv2.resize(crop_img, (sz, sz)).astype(np.uint8)\n",
    "        np.save(save_dir + f'{image_id}', crop_img)\n",
    "    except:\n",
    "        orig_image = cv2.resize(orig_image, (sz, sz)).astype(np.uint8)\n",
    "        np.save(save_dir + f'{image_id}', orig_image)"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:24.59262Z",
     "iopub.execute_input": "2021-12-13T10:53:24.593379Z",
     "iopub.status.idle": "2021-12-13T10:53:24.689624Z",
     "shell.execute_reply.started": "2021-12-13T10:53:24.593337Z",
     "shell.execute_reply": "2021-12-13T10:53:24.688747Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -r /kaggle/working/yolov5"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:24.69124Z",
     "iopub.execute_input": "2021-12-13T10:53:24.691732Z",
     "iopub.status.idle": "2021-12-13T10:53:25.445152Z",
     "shell.execute_reply.started": "2021-12-13T10:53:24.691696Z",
     "shell.execute_reply": "2021-12-13T10:53:25.443784Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(os.listdir(\"./crop_images\")))\n",
    "print(os.listdir(\"./crop_images\")[:1])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:25.447996Z",
     "iopub.execute_input": "2021-12-13T10:53:25.448251Z",
     "iopub.status.idle": "2021-12-13T10:53:25.457125Z",
     "shell.execute_reply.started": "2021-12-13T10:53:25.448218Z",
     "shell.execute_reply": "2021-12-13T10:53:25.456011Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:53:25.459709Z",
     "iopub.execute_input": "2021-12-13T10:53:25.460529Z",
     "iopub.status.idle": "2021-12-13T10:57:20.666147Z",
     "shell.execute_reply.started": "2021-12-13T10:53:25.460487Z",
     "shell.execute_reply": "2021-12-13T10:57:20.664817Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "​\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "​\n",
    "from tqdm.notebook import tqdm\n",
    "import os, gc\n",
    "import random\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import zipfile\n",
    "import collections\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from random import randint\n",
    "from glob import glob\n",
    "import shutil\n",
    "​\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "​\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "​\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "​\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "​\n",
    "seed = 2020\n",
    "seed_everything(seed)\n",
    "sz = 224\n",
    "NFOLDS = 5\n",
    "​\n",
    "#ImageNet\n",
    "mean = np.array([[[0.485, 0.456, 0.406]]])\n",
    "std = np.array([[[0.229, 0.224, 0.225]]])\n",
    "add Codeadd Markdown\n",
    "test_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\n",
    "test_ids = test_df.Id.to_list()\n",
    "test_dir = \"/kaggle/input/petfinder-pawpularity-score/test/\"\n",
    "shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\n",
    "os.chdir('/kaggle/working/yolov5')\n",
    "add Codeadd Markdown\n",
    "!python detect.py\\\n",
    "--weights /kaggle/input/ultralyticsyolov5aweights/yolov5x.pt\\\n",
    "--class 15 16\\\n",
    "--img 512\\\n",
    "--conf 0.3\\\n",
    "--iou 0.5\\\n",
    "--source $test_dir\\\n",
    "--name inference\\\n",
    "--save-txt --save-conf --exist-ok\n",
    "Namespace(agnostic_nms=False, augment=False, classes=[15, 16], conf_thres=0.3, device='', exist_ok=True, img_size=512, iou_thres=0.5, name='inference', project='runs/detect', save_conf=True, save_txt=True, source='/kaggle/input/petfinder-pawpularity-score/test/', update=False, view_img=False, weights=['/kaggle/input/ultralyticsyolov5aweights/yolov5x.pt'])\n",
    "Fusing layers...\n",
    "image 1/8 /kaggle/input/petfinder-pawpularity-score/test/4128bae22183829d2b5fea10effdb0c3.jpg: 512x512 Done. (0.041s)\n",
    "image 2/8 /kaggle/input/petfinder-pawpularity-score/test/43a2262d7738e3d420d453815151079e.jpg: 512x512 Done. (0.033s)\n",
    "image 3/8 /kaggle/input/petfinder-pawpularity-score/test/4e429cead1848a298432a0acad014c9d.jpg: 512x512 Done. (0.031s)\n",
    "image 4/8 /kaggle/input/petfinder-pawpularity-score/test/80bc3ccafcc51b66303c2c263aa38486.jpg: 512x512 Done. (0.031s)\n",
    "image 5/8 /kaggle/input/petfinder-pawpularity-score/test/8f49844c382931444e68dffbe20228f4.jpg: 512x512 Done. (0.030s)\n",
    "image 6/8 /kaggle/input/petfinder-pawpularity-score/test/b03f7041962238a7c9d6537e22f9b017.jpg: 512x512 Done. (0.030s)\n",
    "image 7/8 /kaggle/input/petfinder-pawpularity-score/test/c978013571258ed6d4637f6e8cc9d6a3.jpg: 512x512 Done. (0.030s)\n",
    "image 8/8 /kaggle/input/petfinder-pawpularity-score/test/e0de453c1bffc20c22b072b34b54e50f.jpg: 512x512 Done. (0.030s)\n",
    "Results saved to runs/detect/inference\n",
    "0 labels saved to runs/detect/inference/labels\n",
    "Done. (0.460s)\n",
    "add Codeadd Markdown\n",
    "os.chdir('/kaggle/working')\n",
    "save_dir = f'/kaggle/working/crop_images/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "add Codeadd Markdown\n",
    "for n, image_id in tqdm(enumerate(test_ids)):\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    orig_image = cv2.imread(f'/kaggle/input/petfinder-pawpularity-score/test/{image_id}.jpg')\n",
    "    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "    height = orig_image.shape[0]\n",
    "    width = orig_image.shape[1]\n",
    "    try:\n",
    "        file_path = f'/kaggle/working/yolov5/runs/detect/inference/labels/{image_id}.txt'\n",
    "        f = open(file_path, 'r')\n",
    "        data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n",
    "        data = data[:, [0, 5, 1, 2, 3, 4]]\n",
    "        for i, d in enumerate(data):\n",
    "            xmin.append((d[2]-d[4]/2)*width)\n",
    "            ymin.append((d[3]-d[5]/2)*height)\n",
    "            xmax.append((d[2]+d[4]/2)*width)\n",
    "            ymax.append((d[3]+d[5]/2)*height)\n",
    "        all_xmin = int(min(xmin))\n",
    "        all_ymin = int(min(ymin))\n",
    "        all_xmax = int(max(xmax))\n",
    "        all_ymax = int(max(ymax))\n",
    "        all_width_half = (all_xmax - all_xmin) // 2\n",
    "        all_height_half = (all_ymax - all_ymin) // 2\n",
    "        r = np.maximum(all_width_half, all_height_half)\n",
    "        all_xc = (all_xmin + all_xmax) // 2\n",
    "        all_yc = (all_ymin + all_ymax) // 2\n",
    "        final_xmin = np.maximum(all_xc-r, 0)\n",
    "        final_ymin = np.maximum(all_yc-r, 0)\n",
    "        final_xmax = np.minimum(all_xc+r, width)\n",
    "        final_ymax = np.minimum(all_yc+r, height)\n",
    "        crop_img = orig_image[final_ymin:final_ymax, final_xmin:final_xmax, :]\n",
    "        crop_img = cv2.resize(crop_img, (sz, sz)).astype(np.uint8)\n",
    "        np.save(save_dir + f'{image_id}', crop_img)\n",
    "    except:\n",
    "        orig_image = cv2.resize(orig_image, (sz, sz)).astype(np.uint8)\n",
    "        np.save(save_dir + f'{image_id}', orig_image)\n",
    "8/? [00:00<00:00, 118.62it/s]\n",
    "add Codeadd Markdown\n",
    "!rm -r /kaggle/working/yolov5\n",
    "add Codeadd Markdown\n",
    "print(len(os.listdir(\"./crop_images\")))\n",
    "print(os.listdir(\"./crop_images\")[:1])\n",
    "8\n",
    "['80bc3ccafcc51b66303c2c263aa38486.npy']\n",
    "add Codeadd Markdown\n",
    "\n",
    "import sys\n",
    "​\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "# Asthetics\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "​\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "​\n",
    "# General\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "​\n",
    "pd.set_option('display.max_columns', None)\n",
    "​\n",
    "# Image Aug\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "​\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "​\n",
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 2021\n",
    "​\n",
    "​\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "​\n",
    "​\n",
    "seed_everything()\n",
    "​\n",
    "# Device Optimization\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "​\n",
    "print(f'Using device: {device}')\n",
    "​\n",
    "csv_dir = '../input/petfinder-pawpularity-score'\n",
    "test_dir = './crop_images'\n",
    "models_dir = '../input/swin-transformenrs-pet-net'\n",
    "​\n",
    "test_file_path = os.path.join(csv_dir, 'test.csv')\n",
    "sample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n",
    "print(f'Test file: {test_file_path}')\n",
    "print(f'Models path: {models_dir}')\n",
    "​\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "sample_df = pd.read_csv(sample_sub_file_path)\n",
    "​\n",
    "​\n",
    "def return_filpath(name, folder):\n",
    "    path = os.path.join(folder, f'{name}.npy')\n",
    "    return path\n",
    "​\n",
    "​\n",
    "test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))\n",
    "test_df.head()\n",
    "​\n",
    "df = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\n",
    "print('Train shape:', df.shape)\n",
    "df.head()\n",
    "​\n",
    "params = {\n",
    "    'model': 'swin_large_patch4_window12_384',\n",
    "    'model2': 'swin_large_patch4_window7_224',\n",
    "    'dense_features': ['Subject Focus', 'Eyes', 'Face', 'Near',\n",
    "                       'Action', 'Accessory', 'Group', 'Collage',\n",
    "                       'Human', 'Occlusion', 'Info', 'Blur'],\n",
    "    'pretrained': False,\n",
    "    'inp_channels': 3,\n",
    "    'im_size': 384,\n",
    "    'device': device,\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 0,\n",
    "    'out_features': 1,\n",
    "    'debug': False\n",
    "}\n",
    "if params['debug']:\n",
    "    test_df = test_df.sample(frac=0.1)\n",
    "​\n",
    "​\n",
    "def get_test_transforms(DIM=params['im_size']):\n",
    "    return albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(DIM, DIM),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )\n",
    "​\n",
    "​\n",
    "def get_test_Flip_transforms(DIM=params['im_size']):\n",
    "    return albumentations.Compose(\n",
    "        [albumentations.HorizontalFlip(p=0.5),\n",
    "​\n",
    "         albumentations.Resize(DIM, DIM),\n",
    "         albumentations.Normalize(\n",
    "             mean=[0.485, 0.456, 0.406],\n",
    "             std=[0.229, 0.224, 0.225],\n",
    "         ),\n",
    "         ToTensorV2(p=1.0)\n",
    "         ]\n",
    "    )\n",
    "​\n",
    "def get_test_Flip_vertical_transforms(DIM=params['im_size']):\n",
    "    return albumentations.Compose(\n",
    "        [\n",
    "​\n",
    "         albumentations.VerticalFlip(p=0.5),\n",
    "         albumentations.Resize(DIM, DIM),\n",
    "         albumentations.Normalize(\n",
    "             mean=[0.485, 0.456, 0.406],\n",
    "             std=[0.229, 0.224, 0.225],\n",
    "         ),\n",
    "         ToTensorV2(p=1.0)\n",
    "         ]\n",
    "    )\n",
    "​\n",
    "​\n",
    "def get_test_Shift_Scale_transforms(DIM=params['im_size']):\n",
    "    return albumentations.Compose(\n",
    "        [albumentations.ShiftScaleRotate(\n",
    "            shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n",
    "        ),\n",
    "​\n",
    "            albumentations.Resize(DIM, DIM),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )\n",
    "​\n",
    "​\n",
    "def get_test_Rotate_transforms(DIM=params['im_size']):\n",
    "    return albumentations.Compose(\n",
    "        [albumentations.Rotate(limit=180, p=0.7),\n",
    "         albumentations.Resize(DIM, DIM),\n",
    "         albumentations.Normalize(\n",
    "             mean=[0.485, 0.456, 0.406],\n",
    "             std=[0.229, 0.224, 0.225],\n",
    "         ),\n",
    "         ToTensorV2(p=1.0)\n",
    "         ]\n",
    "    )\n",
    "​\n",
    "​\n",
    "class CuteDataset(Dataset):\n",
    "    def __init__(self, images_filepaths, dense_features, targets, transform=None, transform_flip=None,\n",
    "                 transform_shiftscale=None, transform_rotate=None,transform_flip_vertical=None):\n",
    "        self.images_filepaths = images_filepaths\n",
    "        self.dense_features = dense_features\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.transform_flip = transform_flip\n",
    "        self.transform_shiftscale = transform_shiftscale\n",
    "        self.transform_rotate = transform_rotate\n",
    "        self.transform_vertical_flip = transform_flip_vertical\n",
    "​\n",
    "    def __len__(self):\n",
    "        return len(self.images_filepaths)\n",
    "​\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.images_filepaths[idx]\n",
    "        image = np.load(image_filepath).astype(np.float32)\n",
    "​\n",
    "        if self.transform is not None:\n",
    "            image1 = self.transform(image=image)['image']\n",
    "        if self.transform_flip is not None:\n",
    "            image2 = self.transform_flip(image=image)['image']\n",
    "        if self.transform_shiftscale is not None:\n",
    "            image3 = self.transform_shiftscale(image=image)['image']\n",
    "        if self.transform_shiftscale is not None:\n",
    "            image4 = self.transform_rotate(image=image)['image']\n",
    "        if self.transform_vertical_flip is not None:\n",
    "            image5 = self.transform_vertical_flip(image=image)['image']\n",
    "​\n",
    "        dense = self.dense_features[idx, :]\n",
    "        label = torch.tensor(self.targets[idx]).float()\n",
    "        return image1, image2, image3, image4,image5, dense, label\n",
    "​\n",
    "​\n",
    "class PetNet(nn.Module):\n",
    "    def __init__(self, model_name=params['model'], out_features=params['out_features'],\n",
    "                 inp_channels=params['inp_channels'],\n",
    "                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n",
    "        n_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(n_features, 128)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + num_dense, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_features)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "​\n",
    "    def forward(self, image, dense):\n",
    "        embeddings = self.model(image)\n",
    "        x = self.dropout(embeddings)\n",
    "        x = torch.cat([x, dense], dim=1)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "​\n",
    "​\n",
    "class PetNet2(nn.Module):\n",
    "    def __init__(self, model_name=params['model2'], out_features=params['out_features'],\n",
    "                 inp_channels=params['inp_channels'],\n",
    "                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n",
    "        n_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(n_features, 128)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_features)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "​\n",
    "    def forward(self, image, dense):\n",
    "        embeddings = self.model(image)\n",
    "        x = self.dropout(embeddings)\n",
    "        x = torch.cat([x], dim=1)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "​\n",
    "​\n",
    "​\n",
    "​\n",
    "predicted_labels = None\n",
    "for model_name in glob.glob(\"../input/imagenet-1k-swin-large-384/SwinLarge384Withcrop\" + '/*.pth'):\n",
    "    model = PetNet()\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model = model.to(params['device'])\n",
    "    model.eval()\n",
    "​\n",
    "    test_dataset = CuteDataset(\n",
    "        images_filepaths=test_df['image_path'].values,\n",
    "        dense_features=test_df[params['dense_features']].values,\n",
    "        targets=sample_df['Pawpularity'].values,\n",
    "        transform=get_test_transforms(),\n",
    "        transform_flip=get_test_Flip_transforms(),\n",
    "        transform_shiftscale=get_test_Shift_Scale_transforms(),\n",
    "        transform_rotate=get_test_Rotate_transforms(),\n",
    "        transform_flip_vertical=get_test_Flip_vertical_transforms()\n",
    "​\n",
    "​\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=params['batch_size'],\n",
    "        shuffle=False, num_workers=params['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "​\n",
    "    temp_preds = None\n",
    "    with torch.no_grad():\n",
    "        for (images, images_flip, images_shift, images_rotate,images_flip_vertical, dense, target) in tqdm(test_loader,\n",
    "                                                                                      desc=f'Predicting. '):\n",
    "            images = images.to(params['device'], non_blocking=True)\n",
    "            images_flip = images_flip.to(params['device'], non_blocking=True)\n",
    "            images_shift = images_shift.to(params['device'], non_blocking=True)\n",
    "            images_rotate = images_rotate.to(params['device'], non_blocking=True)\n",
    "            images_flip_vertical = images_flip_vertical.to(params['device'], non_blocking=True)\n",
    "            dense = dense.to(params['device'], non_blocking=True)\n",
    "            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_flip, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_shift, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_rotate, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_flip_vertical, dense)).to('cpu').numpy() * 100\n",
    "​\n",
    "            predictions = predictions / 5\n",
    "            if temp_preds is None:\n",
    "                temp_preds = predictions\n",
    "            else:\n",
    "                temp_preds = np.vstack((temp_preds, predictions))\n",
    "​\n",
    "    if predicted_labels is None:\n",
    "        predicted_labels = temp_preds\n",
    "    else:\n",
    "        predicted_labels += temp_preds\n",
    "​\n",
    "predicted_labels /= (len(glob.glob(\"../input/imagenet-1k-swin-large-384/SwinLarge384Withcrop\" + '/*.pth')))\n",
    "​\n",
    "test_dataset = CuteDataset(\n",
    "    images_filepaths=test_df['image_path'].values,\n",
    "    dense_features=test_df[params['dense_features']].values,\n",
    "    targets=sample_df['Pawpularity'].values,\n",
    "    transform=get_test_transforms(224),\n",
    "    transform_flip=get_test_Flip_transforms(224),\n",
    "    transform_shiftscale=get_test_Shift_Scale_transforms(224),\n",
    "    transform_rotate=get_test_Rotate_transforms(224),\n",
    "    transform_flip_vertical=get_test_Flip_vertical_transforms(224),\n",
    ")\n",
    "​\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=params['batch_size'],\n",
    "    shuffle=False, num_workers=params['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "​\n",
    "predicted_labels2 = None\n",
    "for model_name in glob.glob(\"../input/imagenet-1k-swin/SwinLargw224Nonmixp\" + '/*.pth'):\n",
    "    model = PetNet2()\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model = model.to(params['device'])\n",
    "    model.eval()\n",
    "​\n",
    "    temp_preds = None\n",
    "    with torch.no_grad():\n",
    "        for (images, images_flip, images_shift, images_rotate,images_flip_vertical, dense, target) in tqdm(test_loader,\n",
    "                                                                                      desc=f'Predicting. '):\n",
    "            images = images.to(params['device'], non_blocking=True)\n",
    "            images_flip = images_flip.to(params['device'], non_blocking=True)\n",
    "            images_shift = images_shift.to(params['device'], non_blocking=True)\n",
    "            images_rotate = images_rotate.to(params['device'], non_blocking=True)\n",
    "            images_flip_vertical = images_flip_vertical.to(params['device'], non_blocking=True)\n",
    "            dense = dense.to(params['device'], non_blocking=True)\n",
    "            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_flip, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_shift, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_rotate, dense)).to('cpu').numpy() * 100\n",
    "            predictions += torch.sigmoid(model(images_flip_vertical, dense)).to('cpu').numpy() * 100\n",
    "​\n",
    "            predictions = predictions / 5\n",
    "            if temp_preds is None:\n",
    "                temp_preds = predictions\n",
    "            else:\n",
    "                temp_preds = np.vstack((temp_preds, predictions))\n",
    "​\n",
    "    if predicted_labels2 is None:\n",
    "        predicted_labels2 = temp_preds\n",
    "    else:\n",
    "        predicted_labels2 += temp_preds\n",
    "​\n",
    "#     del model\n",
    "​\n",
    "predicted_labels2 /= (len(glob.glob(\"../input/imagenet-1k-swin/SwinLargw224Nonmixp\" + '/*.pth')))\n",
    "​\n",
    "Using device: cuda\n",
    "Test file: ../input/petfinder-pawpularity-score/test.csv\n",
    "Models path: ../input/swin-transformenrs-pet-net\n",
    "Train shape: (9912, 14)\n",
    "Predicting. : 100%\n",
    "1/1 [00:02<00:00, 2.12s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.97s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.97s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.97s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:02<00:00, 2.11s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.97s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.97s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.97s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.97s/it]\n",
    "Predicting. : 100%\n",
    "1/1 [00:01<00:00, 1.98s/it]\n",
    "add Codeadd Markdown\n",
    "​\n",
    "import sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "# Asthetics\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "​\n",
    "# General\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "pd.set_option('display.max_columns', None)\n",
    "​\n",
    "# Image Aug\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "​\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "​\n",
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 2021\n",
    "​\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything()\n",
    "​\n",
    "# Device Optimization\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "​\n",
    "​\n",
    "csv_dir = '../input/petfinder-pawpularity-score'\n",
    "test_dir = '../input/petfinder-pawpularity-score/test'\n",
    "models_dir = '../input/swin-transformenrs-pet-net'\n",
    "​\n",
    "test_file_path = os.path.join(csv_dir, 'test.csv')\n",
    "sample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n",
    "print(f'Test file: {test_file_path}')\n",
    "print(f'Models path: {models_dir}')\n",
    "​\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "sample_df = pd.read_csv(sample_sub_file_path)\n",
    "​\n",
    "def return_filpath(name, folder):\n",
    "    path = os.path.join(folder, f'{name}.jpg')\n",
    "    return path\n",
    "test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))\n",
    "test_df.head()\n",
    "​\n",
    "df = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\n",
    "print('Train shape:', df.shape )\n",
    "df.head()\n",
    "​\n",
    "params = {\n",
    "    'model': 'swin_large_patch4_window12_384_in22k',\n",
    "    'model2' : 'vit_large_patch16_224_in21k',\n",
    "    'dense_features': ['Subject Focus', 'Eyes', 'Face', 'Near',\n",
    "                       'Action', 'Accessory', 'Group', 'Collage',\n",
    "                       'Human', 'Occlusion', 'Info', 'Blur'],\n",
    "    'pretrained': False,\n",
    "    'inp_channels': 3,\n",
    "    'im_size': 384,\n",
    "    'device': device,\n",
    "    'batch_size': 16,\n",
    "    'num_workers' : 0,\n",
    "    'out_features': 1,\n",
    "    'debug': False\n",
    "}\n",
    "​\n",
    "if params['debug']:\n",
    "    test_df = test_df.sample(frac=0.1)\n",
    "​\n",
    "\n",
    "def get_test_transforms(DIM = params['im_size']):\n",
    "    return albumentations.Compose(\n",
    "        [albumentations.HorizontalFlip(p=0.5),\n",
    "​\n",
    "         albumentations.VerticalFlip(p=0.5),\n",
    "          albumentations.Resize(DIM,DIM),\n",
    "          albumentations.Normalize(\n",
    "              mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225],\n",
    "          ),\n",
    "          ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )\n",
    "​\n",
    "​\n",
    "class CuteDataset(Dataset):\n",
    "    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n",
    "        self.images_filepaths = images_filepaths\n",
    "        self.dense_features = dense_features\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "​\n",
    "    def __len__(self):\n",
    "        return len(self.images_filepaths)\n",
    "​\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.images_filepaths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "​\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        dense = self.dense_features[idx, :]\n",
    "        label = torch.tensor(self.targets[idx]).float()\n",
    "        return image, dense, label\n",
    "​\n",
    "class PetNet(nn.Module):\n",
    "    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'],\n",
    "                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n",
    "        n_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(n_features, 128)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + num_dense, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_features)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, image, dense):\n",
    "        embeddings = self.model(image)\n",
    "        x = self.dropout(embeddings)\n",
    "        x = torch.cat([x, dense], dim=1)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "​\n",
    "test_dataset = CuteDataset(\n",
    "    images_filepaths = test_df['image_path'].values,\n",
    "    dense_features = test_df[params['dense_features']].values,\n",
    "    targets = sample_df['Pawpularity'].values,\n",
    "    transform = get_test_transforms()\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=params['batch_size'],\n",
    "    shuffle=False, num_workers=params['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "​\n",
    "predicted_labels1 = None\n",
    "for model_name in glob.glob(\"../input/swin-transformenrs-pet-net\" + '/*.pth'):\n",
    "    model = PetNet()\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model = model.to(params['device'])\n",
    "    model.eval()\n",
    "​\n",
    "    test_dataset = CuteDataset(\n",
    "        images_filepaths = test_df['image_path'].values,\n",
    "        dense_features = test_df[params['dense_features']].values,\n",
    "        targets = sample_df['Pawpularity'].values,\n",
    "        transform = get_test_transforms()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=params['batch_size'],\n",
    "        shuffle=False, num_workers=params['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "​\n",
    "    temp_preds = None\n",
    "    with torch.no_grad():\n",
    "        for (images, dense, target) in tqdm(test_loader, desc=f'Predicting. '):\n",
    "            images = images.to(params['device'], non_blocking=True)\n",
    "            dense = dense.to(params['device'], non_blocking=True)\n",
    "            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n",
    "\n",
    "            if temp_preds is None:\n",
    "                temp_preds = predictions\n",
    "            else:\n",
    "                temp_preds = np.vstack((temp_preds, predictions))\n",
    "​\n",
    "    if predicted_labels1 is None:\n",
    "        predicted_labels1 = temp_preds\n",
    "    else:\n",
    "        predicted_labels1 += temp_preds\n",
    "\n",
    "predicted_labels1 /= (len(glob.glob(\"../input/swin-transformenrs-pet-net\" + '/*.pth')))\n",
    "add Codeadd Markdown\n",
    "predicted_labels = predicted_labels * 1/3 + predicted_labels2 * 1/3 + predicted_labels1 * 1/3\n",
    "add Codeadd Markdown\n",
    "print(predicted_labels2)\n",
    "add Codeadd Markdown\n",
    "!rm -r /kaggle/working/crop_images\n",
    "add Codeadd Markdown\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df['Id'] = test_df['Id']\n",
    "sub_df['Pawpularity'] = predicted_labels\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "​\n",
    "add Codeadd Markdown\n",
    "​\n",
    "add Codeadd Markdown\n",
    "​\n",
    "add Codeadd Markdown\n",
    "​\n",
    "add Codeadd Markdown"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "predicted_labels = predicted_labels * 1/3 + predicted_labels2 * 1/3 + predicted_labels1 * 1/3",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(predicted_labels2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:58:20.1881Z",
     "iopub.execute_input": "2021-12-13T10:58:20.188993Z",
     "iopub.status.idle": "2021-12-13T10:58:20.195873Z",
     "shell.execute_reply.started": "2021-12-13T10:58:20.188957Z",
     "shell.execute_reply": "2021-12-13T10:58:20.193844Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!rm -r /kaggle/working/crop_images",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:57:20.667535Z",
     "iopub.status.idle": "2021-12-13T10:57:20.668857Z",
     "shell.execute_reply.started": "2021-12-13T10:57:20.668503Z",
     "shell.execute_reply": "2021-12-13T10:57:20.668561Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sub_df = pd.DataFrame()\nsub_df['Id'] = test_df['Id']\nsub_df['Pawpularity'] = predicted_labels\nsub_df.to_csv('submission.csv', index=False)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-13T10:57:20.670113Z",
     "iopub.status.idle": "2021-12-13T10:57:20.672193Z",
     "shell.execute_reply.started": "2021-12-13T10:57:20.671858Z",
     "shell.execute_reply": "2021-12-13T10:57:20.671891Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}