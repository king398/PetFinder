# -*- coding: utf-8 -*-
"""transformers_classifier_method_starter_train_mixup=true.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qB545Ey3yNJ-0aOkhjw4GvoUxp02aY1K

![SETI](https://www.petfinder.my/images/cuteness_meter.jpg)  

# Problem Statement
* Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster.
* With the help of data science, we will accurately determine a pet photoâ€™s appeal to give these rescue animals a higher chance of loving homes.
* Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles.

## Why this competition?
As evident from the problem statement, this competition presents an interesting challenge for a good cause.  
Also (if successful) the solution can be adapted into tools that will can shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and consequently helping animals find a suitable hjome much faster.

## Expected Outcome
Given a photo a pet animal and some basic information about the photo as dense features, we should be able to estimate the 'pawpularity' score of the pet.

## Data Description
Image data is stored in a jpg image format in training folder and the dense features and target scores are mentioned in the `train.csv` file where the Id of each row corresponds to an unique image in the training folder.
There are also some basic info on the photograph as dense features on the `train.csv` file.

## Grading Metric
Submissions are evaluated on **RMSE** between the predicted value and the observed target.

## Problem Category
From the data and objective its is evident that this is a **Regression Problem** in the Computer Vision domain.

**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** ðŸ˜Š
"""

!nvidia - smi
from google.colab import files

files.upload()

!pip
install - -ignore - installed - -upgrade
kaggle
! mkdir
~ /.kaggle
! cp
kaggle.json
~ /.kaggle /
! chmod
600
~ /.kaggle / kaggle.json
!kaggle
competitions
download - c
petfinder - pawpularity - score
!unzip
petfinder - pawpularity - score.zip
!kaggle
datasets
download - d
abhishek / pawpular - folds
!unzip
pawpular - folds.zip
!kaggle
datasets
download - d
phalanx / petfinder2 - cropped - dataset
!unzip
petfinder2 - cropped - dataset.zip
!kaggle
datasets
download - d
mithilsalunkhe / pseudo -
with-old - comp
	!mkdir
pseudo
!unzip / content / pseudo -
with-old - comp.zip - d pseudo

"""# About This Notebook:-
* This notebook tried to demonstrate the use of Transfer learning using Pytorch and how to combine image features with dense features for various tasks.
* We use a vanilla **vit_large_patch32_384** model for extracting image embeddings and concatenate them with the dense features on the last layer on a NN.
* Refer [this link](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094) for description regarding using this particular methodology.
* This notebook only covers the training part. Inference can be found in the notebook link below.

Inference Notebook:- https://www.kaggle.com/manabendrarout/transformers-classifier-method-starter-infer  

<p style='color: #fc0362; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 24px'>TLDR:- We treat this problem as a classification problem by scaling all targets between [0, 1] and use cross entropy loss as loss-function. It is known that transformer based models are performing better than classic CNN based models on this dataset.</p>

# Get GPU Info
"""

!nvidia - smi

"""# Installations"""

!pip
install - qq
timm
!pip
install - qq
albumentations == 1.0
.3
!pip
install - qq
grad - cam
!pip
install - qq
ttach

"""# Imports"""

# Commented out IPython magic to ensure Python compatibility.
# Asthetics
import warnings
import sklearn.exceptions

warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=sklearn.exceptions.UndefinedMetricWarning)

# General
from tqdm.auto import tqdm
from collections import defaultdict
import pandas as pd
import numpy as np
import os
import random
import gc
import cv2
import glob

gc.enable()
pd.set_option('display.max_columns', None)

# Visialisation
import matplotlib.pyplot as plt
# %matplotlib inline

# Image Aug
import albumentations
from albumentations.pytorch.transforms import ToTensorV2

# Deep Learning
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR
import torch
import torchvision
import timm
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable

# Metrics
from sklearn.metrics import mean_squared_error

# Random Seed Initialize
RANDOM_SEED = 42


def seed_everything(seed=RANDOM_SEED):
	os.environ['PYTHONHASHSEED'] = str(seed)
	np.random.seed(seed)
	random.seed(seed)
	torch.manual_seed(seed)
	torch.cuda.manual_seed(seed)
	torch.backends.cudnn.deterministic = True
	torch.backends.cudnn.benchmark = True


seed_everything()

# Device Optimization
if torch.cuda.is_available():
	device = torch.device('cuda')
else:
	device = torch.device('cpu')

print(f'Using device: {device}')

csv_dir = '../input/petfinder-pawpularity-score'
train_dir = "/content/train"
test_dir = '/content/test'

train_file_path = '/content/train_10folds.csv'
sample_sub_file_path = "/content/sample_submission.csv"

print(f'Train file: {train_file_path}')
print(f'Train file: {sample_sub_file_path}')

train_df = pd.read_csv(train_file_path)

test_df = pd.read_csv(sample_sub_file_path)


def return_filpath(name, folder=train_dir):
	path = os.path.join(folder, f'{name}.jpg')
	return path


train_df['image_path'] = train_df['Id'].apply(lambda x: return_filpath(x))
test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))

train_df.head()

test_df.head()

target = ['Pawpularity']
not_features = ['Id', 'kfold', 'image_path', 'Pawpularity']
cols = list(train_df.columns)
features = [feat for feat in cols if feat not in not_features]
print(features)

"""# CFG"""

TRAIN_FOLDS = [5]

params = {
	'model': 'swin_large_patch4_window12_384_in22k',
	'model_1': 'swin_large_patch4_window12_384_in22k',
	'model_2': 'tf_efficientnetv2_m_in21k',
	'dense_features': features,
	'pretrained': True,
	'inp_channels': 3,
	'im_size': 384,
	'device': device,
	'lr': 1e-5,
	'weight_decay': 1e-6,
	'batch_size': 6,
	'num_workers': 0,
	'epochs': 13,
	'out_features': 1,
	'dropout': 0.2,
	'num_fold': 10,
	'mixup': True,
	'mixup_alpha': 1.0,
	'scheduler_name': 'CosineAnnealingWarmRestarts',
	'T_0': 5,
	'T_max': 5,
	'T_mult': 1,
	'min_lr': 1e-7,
	'max_lr': 1e-4
}

"""# Augmentations

There a well known concept called **image augmentations** in CNN. What augmentation generally does is, it artificially increases the dataset size by subtly modifying the existing images to create new ones (while training). One added advantage of this is:- The model becomes more generalized and focuses to finding features and representations rather than completely overfitting to the training data. It also sometimes helps the model train on more noisy data as compared to conventional methods.  

Example:-  
![](https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png)  
Source:- https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png

One of the most popular image augmentation libraries is **Albumentations**. It has an extensive list of image augmentations, the full list can be found in their [documentation](https://albumentations.ai/docs/).  

*Tip:- Not all augmentations are applicable in all conditions. It really depends on the dataset and the problem. Example:- If your task is to identify if a person is standing or sleeping, applying a rotational augmentation can make the model worse.*  

With that in mind, let's define our augmentations:-

## 1. Train Augmentations
"""


def get_train_transforms(DIM=params['im_size']):
	return albumentations.Compose(
		[
			albumentations.Resize(DIM, DIM),
			albumentations.Normalize(
				mean=[0.485, 0.456, 0.406],
				std=[0.229, 0.224, 0.225],
			),
			albumentations.HorizontalFlip(p=0.5),
			albumentations.VerticalFlip(p=0.5),
			albumentations.Rotate(limit=180, p=0.7),
			albumentations.ShiftScaleRotate(
				shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5
			),

			ToTensorV2(p=1.0),
		]
	)


"""## 2. Mixup"""


def mixup_data(x, z, y, params):
	if params['mixup_alpha'] > 0:
		lam = np.random.beta(
			params['mixup_alpha'], params['mixup_alpha']
		)
	else:
		lam = 1

	batch_size = x.size()[0]
	if params['device'].type == 'cuda':
		index = torch.randperm(batch_size).cuda()
	else:
		index = torch.randperm(batch_size)

	mixed_x = lam * x + (1 - lam) * x[index, :]
	mixed_z = lam * z + (1 - lam) * z[index, :]
	y_a, y_b = y, y[index]
	return mixed_x, mixed_z, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
	return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def rand_bbox1(size, lamb):
	""" Generate random bounding box
	Args:
		- size: [width, breadth] of the bounding box
		- lamb: (lambda) cut ratio parameter, sampled from Beta distribution
	Returns:
		- Bounding box
	"""
	W = size[0]
	H = size[1]
	cut_rat = np.sqrt(1. - lamb)
	cut_w = np.int(W * cut_rat)
	cut_h = np.int(H * cut_rat)

	# uniform
	cx = np.random.randint(W)
	cy = np.random.randint(H)

	bbx1 = np.clip(cx - cut_w // 2, 0, W)
	bby1 = np.clip(cy - cut_h // 2, 0, H)
	bbx2 = np.clip(cx + cut_w // 2, 0, W)
	bby2 = np.clip(cy + cut_h // 2, 0, H)

	return bbx1, bby1, bbx2, bby2


def cutmix(image_batch, image_batch_labels, beta=params["mixup_alpha"]):
	""" Generate a CutMix augmented image from a batch
	Args:
		- image_batch: a batch of input images
		- image_batch_labels: labels corresponding to the image batch
		- beta: a parameter of Beta distribution.
	Returns:
		- CutMix image batch, updated labels
	"""
	# generate mixed sample
	lam = np.random.beta(beta, beta)
	rand_index = np.random.permutation(len(image_batch))
	target_a = image_batch_labels
	target_b = image_batch_labels[rand_index]
	bbx1, bby1, bbx2, bby2 = rand_bbox1(image_batch[0].shape, lam)
	image_batch_updated = image_batch.clone()
	image_batch_updated[:, bbx1:bbx2, bby1:bby2, :] = image_batch[rand_index, bbx1:bbx2, bby1:bby2, :]

	# adjust lambda to exactly match pixel ratio
	lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (image_batch.shape[1] * image_batch.shape[2]))
	label = target_a * lam + target_b * (1. - lam)

	return image_batch_updated, label


def rand_bbox(size, lam, center=False, attcen=None):
	if len(size) == 4:
		W = size[2]
		H = size[3]
	elif len(size) == 3:
		W = size[1]
		H = size[2]
	elif len(size) == 2:
		W = size[0]
		H = size[1]
	else:
		raise Exception

	cut_rat = np.sqrt(1. - lam)
	cut_w = np.int(W * cut_rat)
	cut_h = np.int(H * cut_rat)

	if attcen is None:
		# uniform
		cx = 0
		cy = 0
		if W > 0 and H > 0:
			cx = np.random.randint(W)
			cy = np.random.randint(H)
		if center:
			cx = int(W / 2)
			cy = int(H / 2)
	else:
		cx = attcen[0]
		cy = attcen[1]

	bbx1 = np.clip(cx - cut_w // 2, 0, W)
	bby1 = np.clip(cy - cut_h // 2, 0, H)
	bbx2 = np.clip(cx + cut_w // 2, 0, W)
	bby2 = np.clip(cy + cut_h // 2, 0, H)

	return bbx1, bby1, bbx2, bby2


def get_bbox(imgsize=(224, 224), beta=1.0):
	r = np.random.rand(1)
	lam = np.random.beta(beta, beta)
	bbx1, bby1, bbx2, bby2 = rand_bbox(imgsize, lam)

	return [bbx1, bby1, bbx2, bby2]


def get_spm(input, target, model):
	imgsize = (384, 384)
	bs = input.size(0)
	with torch.no_grad():
		output, fms = model(input)
		print(fms.shape + "full")

		clsw = model.fc
		weight = clsw.weight.data
		bias = clsw.bias.data
		weight = weight.view(weight.size(0), weight.size(1), 1, 1)

		fms = F.relu(fms)
		print("relu")
		poolfea = F.adaptive_avg_pool2d(fms, (1, 1)).squeeze()
		print("pool")
		clslogit = F.softmax(clsw.forward(poolfea))
		logitlist = []
		for i in range(bs):
			logitlist.append(clslogit[i, target[i]])
		clslogit = torch.stack(logitlist)

		out = F.conv2d(fms, weight, bias=bias)

		outmaps = []
		for i in range(bs):
			evimap = out[i, target[i]]
			outmaps.append(evimap)

		outmaps = torch.stack(outmaps)
		if imgsize is not None:
			outmaps = outmaps.view(outmaps.size(0), 1, outmaps.size(1), outmaps.size(2))
			outmaps = F.interpolate(outmaps, imgsize, mode='bilinear', align_corners=False)

		outmaps = outmaps.squeeze()

		for i in range(bs):
			outmaps[i] -= outmaps[i].min()
			outmaps[i] /= outmaps[i].sum()

	return outmaps, clslogit


def snapmix(input, target, model=None):
	r = np.random.rand(1)
	lam_a = torch.ones(input.size(0))
	lam_b = 1 - lam_a
	target_b = target.clone()

	if r < 1:
		print("SPM")
		wfmaps, _ = get_spm(input, target, model)
		print("SPM")
		bs = input.size(0)
		lam = np.random.beta(5, 5)
		lam1 = np.random.beta(5, 5)
		rand_index = torch.randperm(bs).cuda()
		wfmaps_b = wfmaps[rand_index, :, :]
		target_b = target[rand_index]

		same_label = target == target_b
		bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)
		bbx1_1, bby1_1, bbx2_1, bby2_1 = rand_bbox(input.size(), lam1)

		area = (bby2 - bby1) * (bbx2 - bbx1)
		area1 = (bby2_1 - bby1_1) * (bbx2_1 - bbx1_1)

		if area1 > 0 and area > 0:
			ncont = input[rand_index, :, bbx1_1:bbx2_1, bby1_1:bby2_1].clone()
			ncont = F.interpolate(ncont, size=(bbx2 - bbx1, bby2 - bby1), mode='bilinear', align_corners=True)
			input[:, :, bbx1:bbx2, bby1:bby2] = ncont
			lam_a = 1 - wfmaps[:, bbx1:bbx2, bby1:bby2].sum(2).sum(1) / (wfmaps.sum(2).sum(1) + 1e-8)
			lam_b = wfmaps_b[:, bbx1_1:bbx2_1, bby1_1:bby2_1].sum(2).sum(1) / (wfmaps_b.sum(2).sum(1) + 1e-8)
			tmp = lam_a.clone()
			lam_a[same_label] += lam_b[same_label]
			lam_b[same_label] += tmp[same_label]
			lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))
			lam_a[torch.isnan(lam_a)] = lam
			lam_b[torch.isnan(lam_b)] = 1 - lam

	return input, target, target_b, lam_a.cuda(), lam_b.cuda()


"""## 3. Valid Augmentations"""


def get_valid_transforms(DIM=params['im_size']):
	return albumentations.Compose(
		[
			albumentations.Resize(DIM, DIM),
			albumentations.Normalize(
				mean=[0.485, 0.456, 0.406],
				std=[0.229, 0.224, 0.225],
			),
			ToTensorV2(p=1.0)
		]
	)


"""# Dataset"""


class CuteDataset(Dataset):
	def __init__(self, images_filepaths, dense_features, targets, transform=None):
		self.images_filepaths = images_filepaths
		self.dense_features = dense_features
		self.targets = targets
		self.transform = transform

	def __len__(self):
		return len(self.images_filepaths)

	def __getitem__(self, idx):
		image_filepath = self.images_filepaths[idx]
		image = cv2.imread(image_filepath)
		image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

		if self.transform is not None:
			image = self.transform(image=image)['image']

		dense = self.dense_features[idx, :]
		label = torch.tensor(self.targets[idx]).float()
		return image, dense, label


class CuteDatasetPseudo(Dataset):
	def __init__(self, images_filepaths, transform=None):
		self.images_filepaths = images_filepaths

		self.transform = transform

	def __len__(self):
		return len(self.images_filepaths)

	def __getitem__(self, idx):
		image_filepath = self.images_filepaths[idx]
		image = cv2.imread(image_filepath)
		image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

		if self.transform is not None:
			image = self.transform(image=image)['image']

		return image


"""## 1. Visualize Some Examples"""

X_train = train_df['image_path']
X_train_dense = train_df[params['dense_features']]
y_train = train_df['Pawpularity']
train_dataset = CuteDataset(
	images_filepaths=X_train.values,
	dense_features=X_train_dense.values,
	targets=y_train.values,
	transform=get_train_transforms()
)


def show_image(train_dataset=train_dataset, inline=4):
	plt.figure(figsize=(20, 10))
	for i in range(inline):
		rand = random.randint(0, len(train_dataset))
		image, dense, label = train_dataset[rand]
		plt.subplot(1, inline, i % inline + 1)
		plt.axis('off')
		plt.imshow(image.permute(2, 1, 0))
		plt.title(f'Pawpularity: {label}')


for i in range(3):
	show_image(inline=4)

del X_train, X_train_dense, y_train, train_dataset

"""# Metrics"""


def usr_rmse_score(output, target):
	y_pred = torch.sigmoid(output).cpu()
	y_pred = y_pred.detach().numpy() * 100
	target = target.cpu() * 100

	return mean_squared_error(target, y_pred, squared=False)


class MetricMonitor:
	def __init__(self, float_precision=3):
		self.float_precision = float_precision
		self.reset()

	def reset(self):
		self.metrics = defaultdict(lambda: {"val": 0, "count": 0, "avg": 0})

	def update(self, metric_name, val):
		metric = self.metrics[metric_name]

		metric["val"] += val
		metric["count"] += 1
		metric["avg"] = metric["val"] / metric["count"]

	def __str__(self):
		return " | ".join(
			[
				"{metric_name}: {avg:.{float_precision}f}".format(
					metric_name=metric_name, avg=metric["avg"],
					float_precision=self.float_precision
				)
				for (metric_name, metric) in self.metrics.items()
			]
		)


"""# Scheduler

Scheduler is essentially an function that changes our learning rate over epochs/steps. But why do we need to do that?
1. The first reason is that our network may become stuck in either saddle points or local minima, and the low learning rate may not be sufficient to break out of the area and descend into areas of the loss landscape with lower loss.
2. Secondly, our model and optimizer may be very sensitive to our initial learning rate choice. If we make a poor initial choice in learning rate, our model may be stuck from the very start.

Instead, we can use Schedulers and specifically Cyclical Learning Rates(CLR) to oscillate our learning rate between upper and lower bounds, enabling us to:
* Have more freedom in our initial learning rate choices.
* Break out of saddle points and local minima.

In practice, using CLRs leads to far fewer learning rate tuning experiments along with near identical accuracy to exhaustive hyperparameter tuning.
"""


def get_scheduler(optimizer, scheduler_params=params):
	if scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':
		scheduler = CosineAnnealingWarmRestarts(
			optimizer,
			T_0=scheduler_params['T_0'],
			eta_min=scheduler_params['min_lr'],
			last_epoch=-1
		)
	elif scheduler_params['scheduler_name'] == 'OneCycleLR':
		scheduler = OneCycleLR(
			optimizer,
			max_lr=scheduler_params['max_lr'],
			steps_per_epoch=int(((scheduler_params['num_fold'] - 1) * train_df.shape[0]) / (
					scheduler_params['num_fold'] * scheduler_params['batch_size'])) + 1,
			epochs=scheduler_params['epochs'],
		)

	elif scheduler_params['scheduler_name'] == 'CosineAnnealingLR':
		scheduler = CosineAnnealingLR(
			optimizer,
			T_max=scheduler_params['T_max'],
			eta_min=scheduler_params['min_lr'],
			last_epoch=-1
		)
	return scheduler


"""# CNN Model

We will inherit from the nn.Module class to define our model. This is a easy as well as effective way of defining the model as it allows very granular control over the complete NN. We are not using the full capability of it here since it is a starter model, but practicing similar definitions will help if/when you decide to play around a little more with the NN layers and functions.  

Also we are using timm for instancing a pre-trained model.  
The complete list of Pytorch pre-trained image models through timm can be found [here](https://rwightman.github.io/pytorch-image-models/)  
"""


class PetNet(nn.Module):
	def __init__(self, model_name=params['model'], out_features=params['out_features'],
	             inp_channels=params['inp_channels'],
	             pretrained=params['pretrained'], num_dense=len(params['dense_features'])):
		super().__init__()
		model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels, num_classes=0)
		self.model = nn.Sequential(*model.children())[:-2]
		self.pool = nn.AdaptiveAvgPool2d((1, 1))
		self.fc = nn.LazyLinear(out_features)
		self.dropout = nn.Dropout(0.2)
		self.Flatten = nn.Flatten()

	def forward(self, image):
		embeddings = self.model(image)
		print("error 1 ")
		x = self.pool(embeddings)
		print("error 2")
		x = self.Flatten(x)
		print("error 3")
		x = self.dropout(x)
		print("error 4")
		x = torch.cat([x], dim=1)
		print("error 5")
		output = self.fc(x)
		print("error 6")
		return output, embeddings

	def get_params(self, param_name):
		ftlayer_params = list(self.model.parameters())
		ftlayer_params_ids = list(map(id, ftlayer_params))
		freshlayer_params = filter(lambda p: id(p) not in ftlayer_params_ids, self.parameters())

		return eval(param_name + '_params')


"""# Train and Validation Functions

## 1. Train Function
"""


def train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler=None):
	metric_monitor = MetricMonitor()
	model.train()
	stream = tqdm(train_loader)
	if_mixup = False
	if_cutmix = True
	if epoch > 40:
		if_mixup = True
	else:
		if_mixup = False

	for i, (images, dense, targets) in enumerate(stream, start=1):
		images = images.to(params['device'], non_blocking=True)
		targets = targets.to(params['device'], non_blocking=True).float().view(-1, 1)
		if if_mixup:
			images, dense, targets_a, targets_b, lam = mixup_data(images, dense, targets.view(-1, 1), params)
			images = images.to(params['device'], dtype=torch.float)
			dense = dense.to(params['device'], dtype=torch.float)
			targets_a = targets_a.to(params['device'], dtype=torch.float)
			targets_b = targets_b.to(params['device'], dtype=torch.float)
		if if_cutmix:
			input, target_a, target_b, lam_a, lam_b = snapmix(images, targets.view(-1, 1), model=model)
			images = images.to(params['device'], dtype=torch.float)
			targets_a = target_a.to(params['device'], dtype=torch.float)
			targets_b = target_b.to(params['device'], dtype=torch.float)
		output, _ = model(images)
		print(output.shape)

		if if_mixup:
			loss = mixup_criterion(criterion, output, targets_a, targets_b, lam)
		if if_cutmix:
			loss_a = criterion(output, target_a)
			loss_b = criterion(output, target_b)
			loss = torch.mean(loss_a * lam_a + loss_b * lam_b)
			print(loss)

		else:
			loss = criterion(output, targets)

		rmse_score = usr_rmse_score(output, targets)

		metric_monitor.update('Loss', loss.item())
		metric_monitor.update('RMSE', rmse_score)
		loss.backward()
		optimizer.step()

		if scheduler is not None:
			scheduler.step()

		optimizer.zero_grad()
		stream.set_description(f"Epoch: {epoch:02}. Train. {metric_monitor}")


from typing import Iterable

import torch
from torch.optim._multi_tensor import SGD

"""## 2. Validate Function"""


def validate_fn(val_loader, model, criterion, epoch, params):
	metric_monitor = MetricMonitor()
	model.eval()
	stream = tqdm(val_loader)
	final_targets = []
	final_outputs = []
	with torch.no_grad():
		for i, (images, dense, targets) in enumerate(stream, start=1):
			images = images.to(params['device'], non_blocking=True)
			target = targets.to(params['device'], non_blocking=True).float().view(-1, 1)
			output, _ = model(images)
			loss = criterion(output, targets)

			rmse_score = usr_rmse_score(output, targets)
			metric_monitor.update('Loss', loss.item())
			metric_monitor.update('RMSE', rmse_score)
			stream.set_description(f"Epoch: {epoch:02}. Valid. {metric_monitor}")

			targets = (target.detach().cpu().numpy() * 100).tolist()
			outputs = (torch.sigmoid(output).detach().cpu().numpy() * 100).tolist()

			final_targets.extend(targets)
			final_outputs.extend(outputs)
	return final_outputs, final_targets


"""# Run"""

best_models_of_each_fold = []
rmse_tracker = []

for fold in TRAIN_FOLDS:
	print(''.join(['#'] * 50))
	print(f"{''.join(['='] * 15)} TRAINING FOLD: {fold + 1}/{train_df['kfold'].nunique()} {''.join(['='] * 15)}")
	# Data Split to train and Validation
	train = train_df[train_df['kfold'] != fold]
	valid = train_df[train_df['kfold'] == fold]

	X_train = train['image_path']
	X_train_dense = train[params['dense_features']]
	y_train = train['Pawpularity'] / 100
	X_valid = valid['image_path']
	X_valid_dense = valid[params['dense_features']]
	y_valid = valid['Pawpularity'] / 100

	# Pytorch Dataset Creation
	train_dataset = CuteDataset(
		images_filepaths=X_train.values,
		dense_features=X_train_dense.values,
		targets=y_train.values,
		transform=get_train_transforms()
	)

	valid_dataset = CuteDataset(
		images_filepaths=X_valid.values,
		dense_features=X_valid_dense.values,
		targets=y_valid.values,
		transform=get_valid_transforms()
	)

	# Pytorch Dataloader creation
	train_loader = DataLoader(
		train_dataset, batch_size=params['batch_size'], shuffle=True,
		num_workers=params['num_workers'], pin_memory=True
	)

	val_loader = DataLoader(
		valid_dataset, batch_size=params['batch_size'], shuffle=False,
		num_workers=params['num_workers'], pin_memory=True
	)

	# Model, cost function and optimizer instancing
	model = PetNet()
	model = model.to(params['device'])
	criterion = nn.BCEWithLogitsLoss()
	optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'],
	                              weight_decay=params['weight_decay'],
	                              amsgrad=False)
	scheduler = get_scheduler(optimizer)

	# Training and Validation Loop
	best_rmse = np.inf
	best_epoch = np.inf
	best_model_name = None
	for epoch in range(1, params['epochs'] + 1):
		train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler)
		predictions, valid_targets = validate_fn(val_loader, model, criterion, epoch, params)
		rmse = round(mean_squared_error(valid_targets, predictions, squared=False), 3)
		if rmse < best_rmse:
			best_rmse = rmse
			best_epoch = epoch
			if best_model_name is not None:
				os.remove(best_model_name)
			torch.save(model.state_dict(),
			           f"{params['model_1']}_{epoch}_epoch_f{fold + 1}_{rmse}_rmse.pth")
			best_model_name = f"{params['model_1']}_{epoch}_epoch_f{fold + 1}_{rmse}_rmse.pth"

	# Print summary of this fold
	print('')
	print(f'The best RMSE: {best_rmse} for fold {fold + 1} was achieved on epoch: {best_epoch}.')
	print(f'The Best saved model is: {best_model_name}')
	best_models_of_each_fold.append(best_model_name)
	rmse_tracker.append(best_rmse)
	print(''.join(['#'] * 50))
	gc.collect()
	torch.cuda.empty_cache()

print('')
print(f'Average RMSE of all folds: {round(np.mean(rmse_tracker), 4)}')

print(targets)

import os

len(os.listdir("/content/pseudo"))

print(output_unlabeled)

from torchsummary import summary

model = timm.create_model("swin_large_patch4_window12_384_in22k", pretrained=True, in_chans=3)
model = model.to(params['device'])
summary(model, (3, 384, 384))

print(torch.save(model.state_dict(), f="name.pth"))

len(features)

from torchsummary import summary

model = timm.create_model("vit_large_patch16_224_in21k", pretrained=True, in_chans=3)
model = model.to(params['device'])
summary(model, (3, 224, 224))

for i, name in enumerate(best_models_of_each_fold):
	print(f'Best model of fold {i + 1}: {name}')

from google.colab import drive

drive.mount('/content/drive')

"""This is a simple starter kernel on implementation of Transfer Learning using Pytorch for this problem. Pytorch has many SOTA Image models which you can try out using the guidelines in this notebook.

I hope you have learnt something from this notebook. I have created this notebook as a baseline model, which you can easily fork and paly-around with to get much better results. I might update parts of it down the line when I get more GPU hours and some interesting ideas.

**If you liked this notebook and use parts of it in you code, please show some support by upvoting this kernel. It keeps me inspired to come-up with such starter kernels and share it with the community.**

Thanks and happy kaggling!
"""
