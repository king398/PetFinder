{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![SETI](https://www.petfinder.my/images/cuteness_meter.jpg)  \n",
    "\n",
    "# Problem Statement\n",
    "* Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster.\n",
    "* With the help of data science, we will accurately determine a pet photoâ€™s appeal to give these rescue animals a higher chance of loving homes.\n",
    "* Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles.\n",
    "\n",
    "## Why this competition?\n",
    "As evident from the problem statement, this competition presents an interesting challenge for a good cause.  \n",
    "Also (if successful) the solution can be adapted into tools that will can shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and consequently helping animals find a suitable hjome much faster.\n",
    "\n",
    "## Expected Outcome\n",
    "Given a photo a pet animal and some basic information about the photo as dense features, we should be able to estimate the 'pawpularity' score of the pet.\n",
    "\n",
    "## Data Description\n",
    "Image data is stored in a jpg image format in training folder and the dense features and target scores are mentioned in the `train.csv` file where the Id of each row corresponds to an unique image in the training folder.\n",
    "There are also some basic info on the photograph as dense features on the `train.csv` file.\n",
    "\n",
    "## Grading Metric\n",
    "Submissions are evaluated on **RMSE** between the predicted value and the observed target.\n",
    "\n",
    "## Problem Category\n",
    "From the data and objective its is evident that this is a **Regression Problem** in the Computer Vision domain.\n",
    "\n",
    "**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** ðŸ˜Š"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# About This Notebook:-\n",
    "* This notebook tried to demonstrate the use of Transfer learning using Pytorch and how to combine image features with dense features for various tasks.\n",
    "* We use a vanilla **vit_large_patch32_384** model for extracting image embeddings and concatenate them with the dense features on the last layer on a NN.\n",
    "* Refer [this link](https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094) for description regarding using this particular methodology.\n",
    "* This notebook only covers the training part. Inference can be found in the notebook link below.\n",
    "\n",
    "Inference Notebook:- https://www.kaggle.com/manabendrarout/transformers-classifier-method-starter-infer  \n",
    "\n",
    "<p style='color: #fc0362; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 24px'>TLDR:- We treat this problem as a classification problem by scaling all targets between [0, 1] and use cross entropy loss as loss-function. It is known that transformer based models are performing better than classic CNN based models on this dataset.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get GPU Info"
   ],
   "metadata": {
    "id": "BINjp9x4_S1v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "7JFphgy7-Bnh",
    "outputId": "9726f4ca-f7a2-45c7-8faa-de794e82430d",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:22:48.897496Z",
     "iopub.execute_input": "2021-10-04T14:22:48.898181Z",
     "iopub.status.idle": "2021-10-04T14:22:50.00688Z",
     "shell.execute_reply.started": "2021-10-04T14:22:48.898046Z",
     "shell.execute_reply": "2021-10-04T14:22:50.00578Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installations"
   ],
   "metadata": {
    "id": "6I-oEtuP_kQ4",
    "_kg_hide-input": false,
    "_kg_hide-output": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qq timm\n",
    "!pip install -qq albumentations == 1.0.3\n",
    "!pip install -qq grad-cam\n",
    "!pip install -qq ttach"
   ],
   "metadata": {
    "id": "9iIVyfJS_Vmb",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2021-10-04T14:22:50.011312Z",
     "iopub.execute_input": "2021-10-04T14:22:50.011562Z",
     "iopub.status.idle": "2021-10-04T14:23:42.878741Z",
     "shell.execute_reply.started": "2021-10-04T14:22:50.011531Z",
     "shell.execute_reply": "2021-10-04T14:23:42.877303Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "id": "OOStUXXcAgDa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Asthetics\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "# General\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "gc.enable()\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Visialisation\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Image Aug\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# Deep Learning\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#Metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Device Optimization\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device('cuda')\n",
    "else:\n",
    "\tdevice = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ],
   "metadata": {
    "id": "fewdVIGwAfpU",
    "outputId": "68c049ba-a995-4305-9788-ee9e6e6fb14e",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:42.881193Z",
     "iopub.execute_input": "2021-10-04T14:23:42.881557Z",
     "iopub.status.idle": "2021-10-04T14:23:51.200338Z",
     "shell.execute_reply.started": "2021-10-04T14:23:42.881495Z",
     "shell.execute_reply": "2021-10-04T14:23:51.199206Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "csv_dir = '../input/petfinder-pawpularity-score'\n",
    "train_dir = '../input/petfinder-pawpularity-score/train'\n",
    "test_dir = '../input/petfinder-pawpularity-score/test'\n",
    "\n",
    "train_file_path = '../input/pawpular-folds/train_5folds.csv'\n",
    "sample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n",
    "\n",
    "print(f'Train file: {train_file_path}')\n",
    "print(f'Train file: {sample_sub_file_path}')"
   ],
   "metadata": {
    "id": "cscftms-CyMn",
    "outputId": "150ac493-d079-4950-85b5-b69f1f3ea8bd",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.202073Z",
     "iopub.execute_input": "2021-10-04T14:23:51.203054Z",
     "iopub.status.idle": "2021-10-04T14:23:51.21224Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.203002Z",
     "shell.execute_reply": "2021-10-04T14:23:51.210933Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(sample_sub_file_path)"
   ],
   "metadata": {
    "id": "B1aBWdnoDWWX",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.216258Z",
     "iopub.execute_input": "2021-10-04T14:23:51.217364Z",
     "iopub.status.idle": "2021-10-04T14:23:51.296319Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.217309Z",
     "shell.execute_reply": "2021-10-04T14:23:51.29533Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def return_filpath(name, folder=train_dir):\n",
    "\tpath = os.path.join(folder, f'{name}.jpg')\n",
    "\treturn path"
   ],
   "metadata": {
    "id": "vy3X2yUODZP1",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.298145Z",
     "iopub.execute_input": "2021-10-04T14:23:51.298483Z",
     "iopub.status.idle": "2021-10-04T14:23:51.304217Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.298442Z",
     "shell.execute_reply": "2021-10-04T14:23:51.30293Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df['image_path'] = train_df['Id'].apply(lambda x: return_filpath(x))\n",
    "test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, folder=test_dir))"
   ],
   "metadata": {
    "id": "4cQlB9XUD31w",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.306344Z",
     "iopub.execute_input": "2021-10-04T14:23:51.307285Z",
     "iopub.status.idle": "2021-10-04T14:23:51.352126Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.307239Z",
     "shell.execute_reply": "2021-10-04T14:23:51.35112Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "id": "7cZ3OJc8D80E",
    "outputId": "3b7f823a-6c3b-484b-b822-03d016b43e17",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.355396Z",
     "iopub.execute_input": "2021-10-04T14:23:51.355667Z",
     "iopub.status.idle": "2021-10-04T14:23:51.386256Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.355624Z",
     "shell.execute_reply": "2021-10-04T14:23:51.385062Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "id": "UI-XjtOiD-T7",
    "outputId": "e896c973-20c6-4030-aca5-eaad2fa53be6",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.388131Z",
     "iopub.execute_input": "2021-10-04T14:23:51.388473Z",
     "iopub.status.idle": "2021-10-04T14:23:51.40241Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.388418Z",
     "shell.execute_reply": "2021-10-04T14:23:51.401045Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "target = ['Pawpularity']\n",
    "not_features = ['Id', 'kfold', 'image_path', 'Pawpularity']\n",
    "cols = list(train_df.columns)\n",
    "features = [feat for feat in cols if feat not in not_features]\n",
    "print(features)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.404344Z",
     "iopub.execute_input": "2021-10-04T14:23:51.405022Z",
     "iopub.status.idle": "2021-10-04T14:23:51.416095Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.404965Z",
     "shell.execute_reply": "2021-10-04T14:23:51.414758Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CFG"
   ],
   "metadata": {
    "id": "GJPLd0HKELDF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TRAIN_FOLDS = [0]"
   ],
   "metadata": {
    "id": "qYYZYyotM7WA",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.418328Z",
     "iopub.execute_input": "2021-10-04T14:23:51.418664Z",
     "iopub.status.idle": "2021-10-04T14:23:51.426093Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.418625Z",
     "shell.execute_reply": "2021-10-04T14:23:51.424705Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "params = {\n",
    "\t'model': 'swin_large_patch4_window12_384',\n",
    "\t'dense_features': features,\n",
    "\t'pretrained': True,\n",
    "\t'inp_channels': 3,\n",
    "\t'im_size': 384,\n",
    "\t'device': device,\n",
    "\t'lr': 1e-5,\n",
    "\t'weight_decay': 1e-6,\n",
    "\t'batch_size': 8,\n",
    "\t'num_workers': 0,\n",
    "\t'epochs': 10,\n",
    "\t'out_features': 1,\n",
    "\t'dropout': 0.2,\n",
    "\t'num_fold': 5,\n",
    "\t'mixup': False,\n",
    "\t'mixup_alpha': 1.0,\n",
    "\t'scheduler_name': 'CosineAnnealingWarmRestarts',\n",
    "\t'T_0': 5,\n",
    "\t'T_max': 5,\n",
    "\t'T_mult': 1,\n",
    "\t'min_lr': 1e-7,\n",
    "\t'max_lr': 1e-4\n",
    "}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.428329Z",
     "iopub.execute_input": "2021-10-04T14:23:51.429064Z",
     "iopub.status.idle": "2021-10-04T14:23:51.438048Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.429022Z",
     "shell.execute_reply": "2021-10-04T14:23:51.436766Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Augmentations\n",
    "\n",
    "There a well known concept called **image augmentations** in CNN. What augmentation generally does is, it artificially increases the dataset size by subtly modifying the existing images to create new ones (while training). One added advantage of this is:- The model becomes more generalized and focuses to finding features and representations rather than completely overfitting to the training data. It also sometimes helps the model train on more noisy data as compared to conventional methods.  \n",
    "\n",
    "Example:-  \n",
    "![](https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png)  \n",
    "Source:- https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png\n",
    "\n",
    "One of the most popular image augmentation libraries is **Albumentations**. It has an extensive list of image augmentations, the full list can be found in their [documentation](https://albumentations.ai/docs/).  \n",
    "\n",
    "*Tip:- Not all augmentations are applicable in all conditions. It really depends on the dataset and the problem. Example:- If your task is to identify if a person is standing or sleeping, applying a rotational augmentation can make the model worse.*  \n",
    "\n",
    "With that in mind, let's define our augmentations:-"
   ],
   "metadata": {
    "id": "DsXs0lrZG6MY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Train Augmentations"
   ],
   "metadata": {
    "id": "ocxEJDymG9-9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_train_transforms(DIM=params['im_size']):\n",
    "\treturn albumentations.Compose(\n",
    "\t\t[\n",
    "\t\t\talbumentations.Resize(DIM, DIM),\n",
    "\t\t\talbumentations.Normalize(\n",
    "\t\t\t\tmean=[0.485, 0.456, 0.406],\n",
    "\t\t\t\tstd=[0.229, 0.224, 0.225],\n",
    "\t\t\t),\n",
    "\t\t\talbumentations.HorizontalFlip(p=0.5),\n",
    "\t\t\talbumentations.VerticalFlip(p=0.5),\n",
    "\t\t\talbumentations.Rotate(limit=180, p=0.7),\n",
    "\t\t\talbumentations.ShiftScaleRotate(\n",
    "\t\t\t\tshift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n",
    "\t\t\t),\n",
    "\t\t\talbumentations.HueSaturationValue(\n",
    "\t\t\t\thue_shift_limit=0.2, sat_shift_limit=0.2,\n",
    "\t\t\t\tval_shift_limit=0.2, p=0.5\n",
    "\t\t\t),\n",
    "\t\t\talbumentations.RandomBrightnessContrast(\n",
    "\t\t\t\tbrightness_limit=(-0.1, 0.1),\n",
    "\t\t\t\tcontrast_limit=(-0.1, 0.1), p=0.5\n",
    "\t\t\t),\n",
    "\t\t\tToTensorV2(p=1.0),\n",
    "\t\t]\n",
    "\t)"
   ],
   "metadata": {
    "id": "VAgcHrtSG53X",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.440846Z",
     "iopub.execute_input": "2021-10-04T14:23:51.441711Z",
     "iopub.status.idle": "2021-10-04T14:23:51.452357Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.441667Z",
     "shell.execute_reply": "2021-10-04T14:23:51.451228Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Mixup"
   ],
   "metadata": {
    "id": "_7Jgmmh0HCI-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def mixup_data(x, z, y, params):\n",
    "\tif params['mixup_alpha'] > 0:\n",
    "\t\tlam = np.random.beta(\n",
    "\t\t\tparams['mixup_alpha'], params['mixup_alpha']\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\tlam = 1\n",
    "\n",
    "\tbatch_size = x.size()[0]\n",
    "\tif params['device'].type == 'cuda':\n",
    "\t\tindex = torch.randperm(batch_size).cuda()\n",
    "\telse:\n",
    "\t\tindex = torch.randperm(batch_size)\n",
    "\n",
    "\tmixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "\tmixed_z = lam * z + (1 - lam) * z[index, :]\n",
    "\ty_a, y_b = y, y[index]\n",
    "\treturn mixed_x, mixed_z, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "\treturn lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ],
   "metadata": {
    "id": "cSI6LYDxHCoy",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.457613Z",
     "iopub.execute_input": "2021-10-04T14:23:51.457845Z",
     "iopub.status.idle": "2021-10-04T14:23:51.468935Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.457818Z",
     "shell.execute_reply": "2021-10-04T14:23:51.467324Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Valid Augmentations"
   ],
   "metadata": {
    "id": "GkfBqo6cHHCZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_valid_transforms(DIM=params['im_size']):\n",
    "\treturn albumentations.Compose(\n",
    "\t\t[\n",
    "\t\t\talbumentations.Resize(DIM, DIM),\n",
    "\t\t\talbumentations.Normalize(\n",
    "\t\t\t\tmean=[0.485, 0.456, 0.406],\n",
    "\t\t\t\tstd=[0.229, 0.224, 0.225],\n",
    "\t\t\t),\n",
    "\t\t\tToTensorV2(p=1.0)\n",
    "\t\t]\n",
    "\t)"
   ],
   "metadata": {
    "id": "vyYpi2CJHHZr",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.470609Z",
     "iopub.execute_input": "2021-10-04T14:23:51.471857Z",
     "iopub.status.idle": "2021-10-04T14:23:51.480231Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.471808Z",
     "shell.execute_reply": "2021-10-04T14:23:51.479052Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "id": "1XzssvuYHNI0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CuteDataset(Dataset):\n",
    "\tdef __init__(self, images_filepaths, dense_features, targets, transform=None):\n",
    "\t\tself.images_filepaths = images_filepaths\n",
    "\t\tself.dense_features = dense_features\n",
    "\t\tself.targets = targets\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images_filepaths)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage_filepath = self.images_filepaths[idx]\n",
    "\t\timage = cv2.imread(image_filepath)\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\tif self.transform is not None:\n",
    "\t\t\timage = self.transform(image=image)['image']\n",
    "\n",
    "\t\tdense = self.dense_features[idx, :]\n",
    "\t\tlabel = torch.tensor(self.targets[idx]).float()\n",
    "\t\treturn image, dense, label"
   ],
   "metadata": {
    "id": "7b4_Zu73HK62",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.482219Z",
     "iopub.execute_input": "2021-10-04T14:23:51.482966Z",
     "iopub.status.idle": "2021-10-04T14:23:51.494099Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.482847Z",
     "shell.execute_reply": "2021-10-04T14:23:51.492908Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Visualize Some Examples"
   ],
   "metadata": {
    "id": "z4LHNKAcTsi7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = train_df['image_path']\n",
    "X_train_dense = train_df[params['dense_features']]\n",
    "y_train = train_df['Pawpularity']\n",
    "train_dataset = CuteDataset(\n",
    "\timages_filepaths=X_train.values,\n",
    "\tdense_features=X_train_dense.values,\n",
    "\ttargets=y_train.values,\n",
    "\ttransform=get_train_transforms()\n",
    ")"
   ],
   "metadata": {
    "id": "-WJ54a68TxVS",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.495514Z",
     "iopub.execute_input": "2021-10-04T14:23:51.496758Z",
     "iopub.status.idle": "2021-10-04T14:23:51.511438Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.496714Z",
     "shell.execute_reply": "2021-10-04T14:23:51.510321Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_image(train_dataset=train_dataset, inline=4):\n",
    "\tplt.figure(figsize=(20, 10))\n",
    "\tfor i in range(inline):\n",
    "\t\trand = random.randint(0, len(train_dataset))\n",
    "\t\timage, dense, label = train_dataset[rand]\n",
    "\t\tplt.subplot(1, inline, i % inline + 1)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(image.permute(2, 1, 0))\n",
    "\t\tplt.title(f'Pawpularity: {label}')"
   ],
   "metadata": {
    "id": "z8r6mz2TVQMA",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.514144Z",
     "iopub.execute_input": "2021-10-04T14:23:51.515106Z",
     "iopub.status.idle": "2021-10-04T14:23:51.524224Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.515062Z",
     "shell.execute_reply": "2021-10-04T14:23:51.522969Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(3):\n",
    "\tshow_image(inline=4)"
   ],
   "metadata": {
    "id": "IVDGvn6uX9du",
    "outputId": "bdf5a8a9-52af-4f7d-ee85-81f537d7ade5",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:51.526288Z",
     "iopub.execute_input": "2021-10-04T14:23:51.526754Z",
     "iopub.status.idle": "2021-10-04T14:23:53.929184Z",
     "shell.execute_reply.started": "2021-10-04T14:23:51.526675Z",
     "shell.execute_reply": "2021-10-04T14:23:53.927439Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del X_train, X_train_dense, y_train, train_dataset"
   ],
   "metadata": {
    "id": "ffF9yW2JUqpq",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:53.931556Z",
     "iopub.execute_input": "2021-10-04T14:23:53.931873Z",
     "iopub.status.idle": "2021-10-04T14:23:53.937472Z",
     "shell.execute_reply.started": "2021-10-04T14:23:53.931833Z",
     "shell.execute_reply": "2021-10-04T14:23:53.936248Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics"
   ],
   "metadata": {
    "id": "GMHo2KKAHw-3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def usr_rmse_score(output, target):\n",
    "\ty_pred = torch.sigmoid(output).cpu()\n",
    "\ty_pred = y_pred.detach().numpy() * 100\n",
    "\ttarget = target.cpu() * 100\n",
    "\n",
    "\treturn mean_squared_error(target, y_pred, squared=False)"
   ],
   "metadata": {
    "id": "nHiDAFprHtD-",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:53.939186Z",
     "iopub.execute_input": "2021-10-04T14:23:53.93983Z",
     "iopub.status.idle": "2021-10-04T14:23:53.95236Z",
     "shell.execute_reply.started": "2021-10-04T14:23:53.939779Z",
     "shell.execute_reply": "2021-10-04T14:23:53.951191Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MetricMonitor:\n",
    "\tdef __init__(self, float_precision=3):\n",
    "\t\tself.float_precision = float_precision\n",
    "\t\tself.reset()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "\tdef update(self, metric_name, val):\n",
    "\t\tmetric = self.metrics[metric_name]\n",
    "\n",
    "\t\tmetric[\"val\"] += val\n",
    "\t\tmetric[\"count\"] += 1\n",
    "\t\tmetric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn \" | \".join(\n",
    "\t\t\t[\n",
    "\t\t\t\t\"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "\t\t\t\t\tmetric_name=metric_name, avg=metric[\"avg\"],\n",
    "\t\t\t\t\tfloat_precision=self.float_precision\n",
    "\t\t\t\t)\n",
    "\t\t\t\tfor (metric_name, metric) in self.metrics.items()\n",
    "\t\t\t]\n",
    "\t\t)"
   ],
   "metadata": {
    "id": "KqMs6f0iIv2e",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:53.954846Z",
     "iopub.execute_input": "2021-10-04T14:23:53.955619Z",
     "iopub.status.idle": "2021-10-04T14:23:53.967386Z",
     "shell.execute_reply.started": "2021-10-04T14:23:53.955574Z",
     "shell.execute_reply": "2021-10-04T14:23:53.965865Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scheduler\n",
    "\n",
    "Scheduler is essentially an function that changes our learning rate over epochs/steps. But why do we need to do that?\n",
    "1. The first reason is that our network may become stuck in either saddle points or local minima, and the low learning rate may not be sufficient to break out of the area and descend into areas of the loss landscape with lower loss.\n",
    "2. Secondly, our model and optimizer may be very sensitive to our initial learning rate choice. If we make a poor initial choice in learning rate, our model may be stuck from the very start.\n",
    "\n",
    "Instead, we can use Schedulers and specifically Cyclical Learning Rates(CLR) to oscillate our learning rate between upper and lower bounds, enabling us to:\n",
    "* Have more freedom in our initial learning rate choices.\n",
    "* Break out of saddle points and local minima.\n",
    "\n",
    "In practice, using CLRs leads to far fewer learning rate tuning experiments along with near identical accuracy to exhaustive hyperparameter tuning."
   ],
   "metadata": {
    "id": "0RLKnQ_XI5fy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_scheduler(optimizer, scheduler_params=params):\n",
    "\tif scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':\n",
    "\t\tscheduler = CosineAnnealingWarmRestarts(\n",
    "\t\t\toptimizer,\n",
    "\t\t\tT_0=scheduler_params['T_0'],\n",
    "\t\t\teta_min=scheduler_params['min_lr'],\n",
    "\t\t\tlast_epoch=-1\n",
    "\t\t)\n",
    "\telif scheduler_params['scheduler_name'] == 'OneCycleLR':\n",
    "\t\tscheduler = OneCycleLR(\n",
    "\t\t\toptimizer,\n",
    "\t\t\tmax_lr=scheduler_params['max_lr'],\n",
    "\t\t\tsteps_per_epoch=int(((scheduler_params['num_fold'] - 1) * train_df.shape[0]) / (\n",
    "\t\t\t\t\t\tscheduler_params['num_fold'] * scheduler_params['batch_size'])) + 1,\n",
    "\t\t\tepochs=scheduler_params['epochs'],\n",
    "\t\t)\n",
    "\n",
    "\telif scheduler_params['scheduler_name'] == 'CosineAnnealingLR':\n",
    "\t\tscheduler = CosineAnnealingLR(\n",
    "\t\t\toptimizer,\n",
    "\t\t\tT_max=scheduler_params['T_max'],\n",
    "\t\t\teta_min=scheduler_params['min_lr'],\n",
    "\t\t\tlast_epoch=-1\n",
    "\t\t)\n",
    "\treturn scheduler"
   ],
   "metadata": {
    "id": "ikY8Rw2vI3fx",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:53.971366Z",
     "iopub.execute_input": "2021-10-04T14:23:53.971777Z",
     "iopub.status.idle": "2021-10-04T14:23:53.984345Z",
     "shell.execute_reply.started": "2021-10-04T14:23:53.971665Z",
     "shell.execute_reply": "2021-10-04T14:23:53.983309Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN Model\n",
    "\n",
    "We will inherit from the nn.Module class to define our model. This is a easy as well as effective way of defining the model as it allows very granular control over the complete NN. We are not using the full capability of it here since it is a starter model, but practicing similar definitions will help if/when you decide to play around a little more with the NN layers and functions.  \n",
    "\n",
    "Also we are using timm for instancing a pre-trained model.  \n",
    "The complete list of Pytorch pre-trained image models through timm can be found [here](https://rwightman.github.io/pytorch-image-models/)  "
   ],
   "metadata": {
    "id": "ZG2lASSbJ7dj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GeM(nn.Module):\n",
    "\tdef __init__(self, p=4, eps=1e-6):\n",
    "\t\tsuper(GeM, self).__init__()\n",
    "\t\tself.p = nn.Parameter(torch.ones(1) * p)\n",
    "\t\tself.eps = eps\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "\tdef gem(self, x, p=3, eps=1e-6):\n",
    "\t\treturn F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(\n",
    "\t\t\tself.eps) + ')'\n",
    "\n",
    "class Petnet(nn.Module):\n",
    "    def __init__(self, model_name=params['model'], out_features=params['out_features'],\n",
    "\t             inp_channels=params['inp_channels'],\n",
    "\t             pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.fc = nn.Linear(self.n_features, self.cfg.target_size)\n",
    "\n",
    "    def feature(self, image):\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ],
   "metadata": {
    "id": "WvxyQ04RJ0Ll",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:53.986343Z",
     "iopub.execute_input": "2021-10-04T14:23:53.986719Z",
     "iopub.status.idle": "2021-10-04T14:23:54.001561Z",
     "shell.execute_reply.started": "2021-10-04T14:23:53.986676Z",
     "shell.execute_reply": "2021-10-04T14:23:54.000489Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and Validation Functions"
   ],
   "metadata": {
    "id": "4t9QFMaWL8UB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Train Function"
   ],
   "metadata": {
    "id": "ICwkAc81L-gl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler=None):\n",
    "\tmetric_monitor = MetricMonitor()\n",
    "\tmodel.train()\n",
    "\tstream = tqdm(train_loader)\n",
    "\n",
    "\tfor i, (images, dense, target) in enumerate(stream, start=1):\n",
    "\t\tif params['mixup']:\n",
    "\t\t\timages, dense, target_a, target_b, lam = mixup_data(images, dense, target.view(-1, 1), params)\n",
    "\t\t\timages = images.to(params['device'], dtype=torch.float)\n",
    "\t\t\tdense = dense.to(params['device'], dtype=torch.float)\n",
    "\t\t\ttarget_a = target_a.to(params['device'], dtype=torch.float)\n",
    "\t\t\ttarget_b = target_b.to(params['device'], dtype=torch.float)\n",
    "\t\telse:\n",
    "\t\t\timages = images.to(params['device'], non_blocking=True)\n",
    "\t\t\tdense = dense.to(params['device'], non_blocking=True)\n",
    "\t\t\ttarget = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n",
    "\n",
    "\t\toutput = model(images, dense)\n",
    "\n",
    "\t\tif params['mixup']:\n",
    "\t\t\tloss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "\t\telse:\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\n",
    "\t\trmse_score = usr_rmse_score(output, target)\n",
    "\t\tmetric_monitor.update('Loss', loss.item())\n",
    "\t\tmetric_monitor.update('RMSE', rmse_score)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif scheduler is not None:\n",
    "\t\t\tscheduler.step()\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tstream.set_description(f\"Epoch: {epoch:02}. Train. {metric_monitor}\")"
   ],
   "metadata": {
    "id": "lPx4HwTbL3_N",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:54.002756Z",
     "iopub.execute_input": "2021-10-04T14:23:54.003045Z",
     "iopub.status.idle": "2021-10-04T14:23:54.020052Z",
     "shell.execute_reply.started": "2021-10-04T14:23:54.003017Z",
     "shell.execute_reply": "2021-10-04T14:23:54.018601Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Validate Function"
   ],
   "metadata": {
    "id": "wKVjdmIbMrN4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def validate_fn(val_loader, model, criterion, epoch, params):\n",
    "\tmetric_monitor = MetricMonitor()\n",
    "\tmodel.eval()\n",
    "\tstream = tqdm(val_loader)\n",
    "\tfinal_targets = []\n",
    "\tfinal_outputs = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i, (images, dense, target) in enumerate(stream, start=1):\n",
    "\t\t\timages = images.to(params['device'], non_blocking=True)\n",
    "\t\t\tdense = dense.to(params['device'], non_blocking=True)\n",
    "\t\t\ttarget = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n",
    "\t\t\toutput = model(images, dense)\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\t\trmse_score = usr_rmse_score(output, target)\n",
    "\t\t\tmetric_monitor.update('Loss', loss.item())\n",
    "\t\t\tmetric_monitor.update('RMSE', rmse_score)\n",
    "\t\t\tstream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n",
    "\n",
    "\t\t\ttargets = (target.detach().cpu().numpy() * 100).tolist()\n",
    "\t\t\toutputs = (torch.sigmoid(output).detach().cpu().numpy() * 100).tolist()\n",
    "\n",
    "\t\t\tfinal_targets.extend(targets)\n",
    "\t\t\tfinal_outputs.extend(outputs)\n",
    "\treturn final_outputs, final_targets"
   ],
   "metadata": {
    "id": "d07oClh-MnzD",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:54.022391Z",
     "iopub.execute_input": "2021-10-04T14:23:54.022771Z",
     "iopub.status.idle": "2021-10-04T14:23:54.037506Z",
     "shell.execute_reply.started": "2021-10-04T14:23:54.022727Z",
     "shell.execute_reply": "2021-10-04T14:23:54.036312Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run"
   ],
   "metadata": {
    "id": "dlPu51_fMysB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "best_models_of_each_fold = []\n",
    "rmse_tracker = []"
   ],
   "metadata": {
    "id": "hmgk5aQ1MwmG",
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:54.040837Z",
     "iopub.execute_input": "2021-10-04T14:23:54.041509Z",
     "iopub.status.idle": "2021-10-04T14:23:54.05301Z",
     "shell.execute_reply.started": "2021-10-04T14:23:54.041465Z",
     "shell.execute_reply": "2021-10-04T14:23:54.051928Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for fold in TRAIN_FOLDS:\n",
    "\tprint(''.join(['#'] * 50))\n",
    "\tprint(f\"{''.join(['='] * 15)} TRAINING FOLD: {fold + 1}/{train_df['kfold'].nunique()} {''.join(['='] * 15)}\")\n",
    "\t# Data Split to train and Validation\n",
    "\ttrain = train_df[train_df['kfold'] != fold]\n",
    "\tvalid = train_df[train_df['kfold'] == fold]\n",
    "\n",
    "\tX_train = train['image_path']\n",
    "\tX_train_dense = train[params['dense_features']]\n",
    "\ty_train = train['Pawpularity'] / 100\n",
    "\tX_valid = valid['image_path']\n",
    "\tX_valid_dense = valid[params['dense_features']]\n",
    "\ty_valid = valid['Pawpularity'] / 100\n",
    "\n",
    "\t# Pytorch Dataset Creation\n",
    "\ttrain_dataset = CuteDataset(\n",
    "\t\timages_filepaths=X_train.values,\n",
    "\t\tdense_features=X_train_dense.values,\n",
    "\t\ttargets=y_train.values,\n",
    "\t\ttransform=get_train_transforms()\n",
    "\t)\n",
    "\n",
    "\tvalid_dataset = CuteDataset(\n",
    "\t\timages_filepaths=X_valid.values,\n",
    "\t\tdense_features=X_valid_dense.values,\n",
    "\t\ttargets=y_valid.values,\n",
    "\t\ttransform=get_valid_transforms()\n",
    "\t)\n",
    "\n",
    "\t# Pytorch Dataloader creation\n",
    "\ttrain_loader = DataLoader(\n",
    "\t\ttrain_dataset, batch_size=params['batch_size'], shuffle=True,\n",
    "\t\tnum_workers=params['num_workers'], pin_memory=True\n",
    "\t)\n",
    "\n",
    "\tval_loader = DataLoader(\n",
    "\t\tvalid_dataset, batch_size=params['batch_size'], shuffle=False,\n",
    "\t\tnum_workers=params['num_workers'], pin_memory=True\n",
    "\t)\n",
    "\n",
    "\t# Model, cost function and optimizer instancing\n",
    "\tmodel = PetNet()\n",
    "\tmodel = model.to(params['device'])\n",
    "\tcriterion = nn.BCEWithLogitsLoss()\n",
    "\toptimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'],\n",
    "\t                              weight_decay=params['weight_decay'],\n",
    "\t                              amsgrad=False)\n",
    "\tscheduler = get_scheduler(optimizer)\n",
    "\n",
    "\t# Training and Validation Loop\n",
    "\tbest_rmse = np.inf\n",
    "\tbest_epoch = np.inf\n",
    "\tbest_model_name = None\n",
    "\tfor epoch in range(1, params['epochs'] + 1):\n",
    "\t\ttrain_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler)\n",
    "\t\tpredictions, valid_targets = validate_fn(val_loader, model, criterion, epoch, params)\n",
    "\t\trmse = round(mean_squared_error(valid_targets, predictions, squared=False), 3)\n",
    "\t\tif rmse < best_rmse:\n",
    "\t\t\tbest_rmse = rmse\n",
    "\t\t\tbest_epoch = epoch\n",
    "\t\t\tif best_model_name is not None:\n",
    "\t\t\t\tos.remove(best_model_name)\n",
    "\t\t\ttorch.save(model.state_dict(),\n",
    "\t\t\t           f\"{params['model']}_{epoch}_epoch_f{fold + 1}_{rmse}_rmse.pth\")\n",
    "\t\t\tbest_model_name = f\"{params['model']}_{epoch}_epoch_f{fold + 1}_{rmse}_rmse.pth\"\n",
    "\n",
    "\t# Print summary of this fold\n",
    "\tprint('')\n",
    "\tprint(f'The best RMSE: {best_rmse} for fold {fold + 1} was achieved on epoch: {best_epoch}.')\n",
    "\tprint(f'The Best saved model is: {best_model_name}')\n",
    "\tbest_models_of_each_fold.append(best_model_name)\n",
    "\trmse_tracker.append(best_rmse)\n",
    "\tprint(''.join(['#'] * 50))\n",
    "\tdel model\n",
    "\tgc.collect()\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "print('')\n",
    "print(f'Average RMSE of all folds: {round(np.mean(rmse_tracker), 4)}')"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2021-10-04T14:23:54.055106Z",
     "iopub.execute_input": "2021-10-04T14:23:54.055461Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, name in enumerate(best_models_of_each_fold):\n",
    "\tprint(f'Best model of fold {i + 1}: {name}')"
   ],
   "metadata": {
    "id": "DLW11PItOY0Z",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "This is a simple starter kernel on implementation of Transfer Learning using Pytorch for this problem. Pytorch has many SOTA Image models which you can try out using the guidelines in this notebook.\n\nI hope you have learnt something from this notebook. I have created this notebook as a baseline model, which you can easily fork and paly-around with to get much better results. I might update parts of it down the line when I get more GPU hours and some interesting ideas.\n\n**If you liked this notebook and use parts of it in you code, please show some support by upvoting this kernel. It keeps me inspired to come-up with such starter kernels and share it with the community.**\n\nThanks and happy kaggling!",
   "metadata": {}
  }
 ]
}