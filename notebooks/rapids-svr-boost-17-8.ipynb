{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Boost CV LB +0.2 with RAPIDS SVR Head\nIn this notebook, we demonstrate how to add a RAPIDS SVR second head to an already trained CNN or Image Transformer with first head. This boosts CV LB by +0.2! This trick was used in CommonLit Comp [here][3] to boost our team into Gold Medal!\n\nWe begin with [Abhishek's][2] public notebook [here][1] which already contains a fully trained Image Transformer model (with NN head). Now in this notebook, we extract the image embeddings (from the trained fold models) and train additional RAPIDS SVR heads for each fold. The original NN head achieves overall CV RSME 18.0 and the new RAPIDS SVR head achieves overall CV RSME 18.0. Both heads are very diverse because the NN head uses Classification (BCE) loss and the SVR head uses Regression loss. During inference, we predict with both heads. When we average both heads' predictions, we achieve overall CV RSME 17.8!\n\nThe technique illustrated here can be applied to any trained image (or NLP) model for CV LB boost! In the first version of this notebook, we train the SVR heads and save the fold models. Then, in later notebook versions and during Kaggle submission, we load the saved SVR models (from this notebook's version 1 which was made into a Kaggle dataset).\n\n[1]: https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n[2]: https://www.kaggle.com/abhishek\n[3]: https://www.kaggle.com/c/commonlitreadabilityprize/discussion/260800","metadata":{"papermill":{"duration":0.009855,"end_time":"2021-10-05T01:29:50.720532","exception":false,"start_time":"2021-10-05T01:29:50.710677","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# How to Add RAPIDS SVR Head\nThere are 3 steps to building a double headed model. The first step is to train your Image NN backbone and head. This was done by Abhishek in his notebook [here][1] and achieves CV RSME 18.0. The next step is to train our RAPIDS SVR head with extracted embeddings from frozen Image NN backbone. This is done in version 1 of notebook you are reading [here][2] and achieves CV RSME 18.0. Lastly, we infer with both heads and average the predictions. This is done in the notebook you are reading and achieves CV RSME 17.8!\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st1.png)\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st2.png)\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st3.png)\n\n[1]: https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n[2]: https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8?scriptVersionId=76282086","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries","metadata":{"papermill":{"duration":0.008276,"end_time":"2021-10-05T01:29:50.73785","exception":false,"start_time":"2021-10-05T01:29:50.729574","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# based on the post here: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\nsys.path.append(\"../input/timmmaster/\")\n\nimport tez\nimport albumentations\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport timm\nimport torch.nn as nn\nfrom sklearn import metrics\nimport torch\nfrom tez.callbacks import EarlyStopping\nfrom tqdm import tqdm\nimport math\nimport glob\n\nclass args:\n    batch_size = 16\n    image_size = 384\n    \ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))","metadata":{"_kg_hide-input":true,"papermill":{"duration":8.607495,"end_time":"2021-10-05T01:29:59.353831","exception":false,"start_time":"2021-10-05T01:29:50.746336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-21T12:40:50.775093Z","iopub.execute_input":"2021-10-21T12:40:50.77581Z","iopub.status.idle":"2021-10-21T12:40:50.783907Z","shell.execute_reply.started":"2021-10-21T12:40:50.775773Z","shell.execute_reply":"2021-10-21T12:40:50.783208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Swim Model and Swim Dataset","metadata":{"papermill":{"duration":0.008453,"end_time":"2021-10-05T01:29:59.371878","exception":false,"start_time":"2021-10-05T01:29:59.363425","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class PawpularDataset:\n    def __init__(self, image_paths, dense_features, targets, augmentations):\n        self.image_paths = image_paths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):\n        image = cv2.imread(self.image_paths[item])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        features = self.dense_features[item, :]\n        targets = self.targets[item]\n        \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"features\": torch.tensor(features, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.float),\n        }\n    \nclass PawpularModel(tez.Model):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False, in_chans=3)\n        self.model.head = nn.Linear(self.model.head.in_features, 128)\n        self.dropout = nn.Dropout(0.1)\n        self.dense1 = nn.Linear(140, 64)\n        self.dense2 = nn.Linear(64, 1)\n\n    def forward(self, image, features, targets=None):\n        x1 = self.model(image)\n        x = self.dropout(x1)\n        x = torch.cat([x, features], dim=1)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        \n        x = torch.cat([x, x1, features], dim=1)\n        return x, 0, {}\n    \ntest_aug = albumentations.Compose(\n    [\n        albumentations.Resize(args.image_size, args.image_size, p=1),\n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.028308,"end_time":"2021-10-05T01:29:59.409017","exception":false,"start_time":"2021-10-05T01:29:59.380709","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-21T12:40:50.786236Z","iopub.execute_input":"2021-10-21T12:40:50.786893Z","iopub.status.idle":"2021-10-21T12:40:50.803209Z","shell.execute_reply.started":"2021-10-21T12:40:50.786853Z","shell.execute_reply":"2021-10-21T12:40:50.802408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import RAPIDS","metadata":{"papermill":{"duration":0.00919,"end_time":"2021-10-05T01:29:59.427243","exception":false,"start_time":"2021-10-05T01:29:59.418053","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import cuml, pickle\nfrom cuml.svm import SVR\nprint('RAPIDS version',cuml.__version__,'\\n')\n\nLOAD_SVR_FROM_PATH = '../input/svr-models-10-folds/'\n\ndf = pd.read_csv('../input/same-old-creating-folds/train_10folds.csv')\nprint('Train shape:', df.shape )\ndf.head()","metadata":{"papermill":{"duration":3.453807,"end_time":"2021-10-05T01:30:02.890319","exception":false,"start_time":"2021-10-05T01:29:59.436512","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-21T12:40:50.804473Z","iopub.execute_input":"2021-10-21T12:40:50.804865Z","iopub.status.idle":"2021-10-21T12:40:50.843184Z","shell.execute_reply.started":"2021-10-21T12:40:50.80477Z","shell.execute_reply":"2021-10-21T12:40:50.842489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test and OOF\nIn version 1 of this notebook, we extract train embeddings and train RAPIDS SVR heads. (Click version 1 to see this). In later versions and during Kaggle submit, we load these saved RAPIDS SVR fold models and just infer data (without training anything).","metadata":{"papermill":{"duration":0.00976,"end_time":"2021-10-05T01:30:02.910957","exception":false,"start_time":"2021-10-05T01:30:02.901197","status":"completed"},"tags":[]}},{"cell_type":"code","source":"super_final_predictions = []\nsuper_final_predictions2 = []\nsuper_final_oof_predictions = []\nsuper_final_oof_predictions2 = []\nsuper_final_oof_true = []\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfor model_name in glob.glob(\"../input/swin-transformenrs-pet-net\" + '/*.pth'):\n    fold_ = 0\n\n    print('#'*25)\n\n    print('#'*25)\n    \n    model = PawpularModel(model_name=\"swin_large_patch4_window12_384_in22k\")\n    model.load(model_name, device=\"cuda\", weights_only=True)\n    \n    df_test = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n    test_img_paths = [f\"../input/petfinder-pawpularity-score/test/{x}.jpg\" for x in df_test[\"Id\"].values]\n        \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)#.iloc[:160]\n    valid_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_valid[\"Id\"].values]\n\n    dense_features = [\n        'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n        'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n    ]\n    \n    name = f\"SVR_fold_{fold_}.pkl\" \n    if LOAD_SVR_FROM_PATH is None:\n        ##################\n        # EXTRACT TRAIN EMBEDDINGS\n        \n        df_train = df[df.kfold != fold_].reset_index(drop=True)#.iloc[:320]\n        train_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_train[\"Id\"].values]\n        \n        train_dataset = PawpularDataset(\n            image_paths=train_img_paths,\n            dense_features=df_train[dense_features].values,\n            targets=df_train['Pawpularity'].values/100.0,\n            augmentations=test_aug,\n        )\n        print('Extracting train embedding...')\n        train_predictions = model.predict(train_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n    \n        embed = np.array([]).reshape((0,128+12))\n        for preds in train_predictions:\n            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n        \n        ##################\n        # FIT RAPIDS SVR\n        print('Fitting SVR...')\n        clf = SVR(C=20.0)\n        clf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n    \n        ##################\n        # SAVE RAPIDS SVR \n        pickle.dump(clf, open(name, \"wb\"))\n        \n    else:\n        ##################\n        # LOAD RAPIDS SVR \n        print('Loading SVR...',LOAD_SVR_FROM_PATH+name)\n        clf = pickle.load(open(LOAD_SVR_FROM_PATH+name, \"rb\"))\n\n    ##################\n    # TEST PREDICTIONS\n    test_dataset = PawpularDataset(\n        image_paths=test_img_paths,\n        dense_features=df_test[dense_features].values,\n        targets=np.ones(len(test_img_paths)),\n        augmentations=test_aug,\n    )\n    print('Predicting test...')\n    test_predictions = model.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_test_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in test_predictions: #tqdm\n        final_test_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_test_predictions = [sigmoid(x) * 100 for x in final_test_predictions]\n    final_test_predictions2 = clf.predict(embed)\n    super_final_predictions.append(final_test_predictions)\n    super_final_predictions2.append(final_test_predictions2)\n    ##################\n    \n    ##################\n    # OOF PREDICTIONS\n    valid_dataset = PawpularDataset(\n        image_paths=valid_img_paths,\n        dense_features=df_valid[dense_features].values,\n        targets=df_valid['Pawpularity'].values/100.0,\n        augmentations=test_aug,\n    )\n    print('Predicting oof...')\n    valid_predictions = model.predict(valid_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_oof_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in valid_predictions:\n        final_oof_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_oof_predictions = [sigmoid(x) * 100 for x in final_oof_predictions]\n    final_oof_predictions2 = clf.predict(embed)    \n    super_final_oof_predictions.append(final_oof_predictions)\n    super_final_oof_predictions2.append(final_oof_predictions2)\n    \n    final_oof_true = df_valid['Pawpularity'].values\n    super_final_oof_true.append(final_oof_true)\n    ##################\n    \n    ##################\n    # COMPUTE RSME\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1]))**2.0 ) )\n    print('NN RSME =',rsme,'\\n')\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1]))**2.0 ) )\n    print('SVR RSME =',rsme,'\\n')\n    \n    w = 0.5\n    oof2 = (1-w)*np.array(super_final_oof_predictions[-1]) + w*np.array(super_final_oof_predictions2[-1])\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - oof2)**2.0 ) )\n    print('Ensemble RSME =',rsme,'\\n')\n    fold +=1","metadata":{"papermill":{"duration":569.218368,"end_time":"2021-10-05T01:39:32.138933","exception":false,"start_time":"2021-10-05T01:30:02.920565","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-21T12:40:50.957727Z","iopub.execute_input":"2021-10-21T12:40:50.95792Z","iopub.status.idle":"2021-10-21T12:40:54.108603Z","shell.execute_reply.started":"2021-10-21T12:40:50.957897Z","shell.execute_reply":"2021-10-21T12:40:54.10745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute CV Score\nBelow we compute the overall CV RSME scores of just the NN head, just the SVR head, and an ensemble of 50% NN and 50% SVR heads. Then we plot all ensemble weights to find the optimal weights for NN head and SVR heads.","metadata":{"papermill":{"duration":0.194813,"end_time":"2021-10-05T01:39:32.538415","exception":false,"start_time":"2021-10-05T01:39:32.343602","status":"completed"},"tags":[]}},{"cell_type":"code","source":"true = np.hstack(super_final_oof_true)\n\noof = np.hstack(super_final_oof_predictions)\nrsme = np.sqrt( np.mean( (oof - true)**2.0 ))\nprint('Overall CV NN head RSME =',rsme)\n\noof2 = np.hstack(super_final_oof_predictions2)\nrsme = np.sqrt( np.mean( (oof2 - true)**2.0 ))\nprint('Overall CV SVR head RSME =',rsme)\n\noof3 = (1-w)*oof + w*oof2\nrsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\nprint('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =',rsme)","metadata":{"papermill":{"duration":0.234202,"end_time":"2021-10-05T01:39:32.976703","exception":false,"start_time":"2021-10-05T01:39:32.742501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-21T12:40:54.109902Z","iopub.status.idle":"2021-10-21T12:40:54.110576Z","shell.execute_reply.started":"2021-10-21T12:40:54.110268Z","shell.execute_reply":"2021-10-21T12:40:54.110298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nscore = []\nfor ww in np.arange(0,1.05,0.05):\n    oof3 = (1-ww)*oof + ww*oof2\n    rsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\n    #print(f'{ww:0.2} CV Ensemble RSME =',rsme)\n    score.append(rsme)\nbest_w = np.argmin(score)*0.05\n\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(21)/20.0,score,'-o')\nplt.plot([best_w],np.min(score),'o',color='black',markersize=15)\nplt.title(f'Best Overall CV RSME={np.min(score):.4} with SVR Ensemble Weight={best_w:.2}',size=16)\nplt.ylabel('Overall Ensemble RSME',size=14)\nplt.xlabel('SVR Weight',size=14)\nplt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.473663,"end_time":"2021-10-05T01:39:33.662871","exception":false,"start_time":"2021-10-05T01:39:33.189208","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-21T12:40:54.111899Z","iopub.status.idle":"2021-10-21T12:40:54.112528Z","shell.execute_reply.started":"2021-10-21T12:40:54.112242Z","shell.execute_reply":"2021-10-21T12:40:54.112268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trust CV or LB?\nAbove we see that using 50% NN head and 50% SVR head achieves the best overall CV score. However our RAPIDS SVR head isn't helping public LB much. We also notice that our RAPIDS SVR head helped folds `1, 2, 4, 5, 7, 8, 9, 10` but did not help folds `3, 6`. So is public test data just a \"bad fold\"? Will our RAPIDS SVR head help private LB? Below we force the weight of SVR head to be 10% in order to achieve a slight public LB boost. But maybe for final submission, we should use 50%??","metadata":{}},{"cell_type":"code","source":"# FORCE SVR WEIGHT TO LOWER VALUE TO HELP PUBLIC LB\nbest_w = 0.2","metadata":{"execution":{"iopub.status.busy":"2021-10-21T12:40:54.113687Z","iopub.status.idle":"2021-10-21T12:40:54.114286Z","shell.execute_reply.started":"2021-10-21T12:40:54.114023Z","shell.execute_reply":"2021-10-21T12:40:54.114048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission CSV\nWe make a submission csv using an ensemble of both heads. We use the optimal ensemble weights that we discovered above.","metadata":{"papermill":{"duration":0.209965,"end_time":"2021-10-05T01:39:34.083427","exception":false,"start_time":"2021-10-05T01:39:33.873462","status":"completed"},"tags":[]}},{"cell_type":"code","source":"super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\nsuper_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\ndf_test[\"Pawpularity\"] = (1-best_w)*super_final_predictions + best_w*super_final_predictions2\ndf_test = df_test[[\"Id\", \"Pawpularity\"]]\ndf_test.to_csv(\"submission.csv\", index=False)\ndf_test.head()","metadata":{"papermill":{"duration":0.228278,"end_time":"2021-10-05T01:39:34.518785","exception":false,"start_time":"2021-10-05T01:39:34.290507","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-10-21T12:40:54.115443Z","iopub.status.idle":"2021-10-21T12:40:54.11606Z","shell.execute_reply.started":"2021-10-21T12:40:54.115805Z","shell.execute_reply":"2021-10-21T12:40:54.11583Z"},"trusted":true},"execution_count":null,"outputs":[]}]}