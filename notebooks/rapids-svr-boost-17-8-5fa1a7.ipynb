{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Boost CV LB +0.2 with RAPIDS SVR Head\n",
    "In this notebook, we demonstrate how to add a RAPIDS SVR second head to an already trained CNN or Image Transformer with first head. This boosts CV LB by +0.2! This trick was used in CommonLit Comp [here][3] to boost our team into Gold Medal!\n",
    "\n",
    "We begin with [Abhishek's][2] public notebook [here][1] which already contains a fully trained Image Transformer model (with NN head). Now in this notebook, we extract the image embeddings (from the trained fold models) and train additional RAPIDS SVR heads for each fold. The original NN head achieves overall CV RSME 18.0 and the new RAPIDS SVR head achieves overall CV RSME 18.0. Both heads are very diverse because the NN head uses Classification (BCE) loss and the SVR head uses Regression loss. During inference, we predict with both heads. When we average both heads' predictions, we achieve overall CV RSME 17.8!\n",
    "\n",
    "The technique illustrated here can be applied to any trained image (or NLP) model for CV LB boost! In the first version of this notebook, we train the SVR heads and save the fold models. Then, in later notebook versions and during Kaggle submission, we load the saved SVR models (from this notebook's version 1 which was made into a Kaggle dataset).\n",
    "\n",
    "[1]: https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n",
    "[2]: https://www.kaggle.com/abhishek\n",
    "[3]: https://www.kaggle.com/c/commonlitreadabilityprize/discussion/260800"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# based on the post here: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../input/tez-lib/\")\n",
    "sys.path.append(\"../input/timmmaster/\")\n",
    "\n",
    "import tez\n",
    "import albumentations\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from tez.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "# Asthetics\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "# General\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Image Aug\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Machine Learning\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Device Optimization\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device('cuda')\n",
    "else:\n",
    "\tdevice = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "class args:\n",
    "\tbatch_size = 16\n",
    "\timage_size = 384\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "\treturn 1 / (1 + math.exp(-x))"
   ],
   "metadata": {
    "_kg_hide-input": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Swim Model and Swim Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "csv_dir = '../input/petfinder-pawpularity-score'\n",
    "test_dir = '../input/petfinder-pawpularity-score/test'\n",
    "models_dir = '../input/swin-transformenrs-pet-net'\n",
    "\n",
    "test_file_path = os.path.join(csv_dir, 'test.csv')\n",
    "sample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n",
    "print(f'Test file: {test_file_path}')\n",
    "print(f'Models path: {models_dir}')\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "sample_df = pd.read_csv(sample_sub_file_path)\n",
    "target = ['Pawpularity']\n",
    "not_features = ['Id', 'kfold', 'image_path', 'Pawpularity']\n",
    "cols = list(test_df.columns)\n",
    "features = [feat for feat in cols if feat not in not_features]\n",
    "print(features)\n",
    "params = {\n",
    "\t'model': 'swin_large_patch4_window12_384_in22k',\n",
    "\t'dense_features': 12,\n",
    "\t'pretrained': False,\n",
    "\t'inp_channels': 3,\n",
    "\t'im_size': 384,\n",
    "\t'device': device,\n",
    "\t'batch_size': 8,\n",
    "\t'num_workers': 2,\n",
    "\t'out_features': 1,\n",
    "\t'debug': False\n",
    "}\n",
    "\n",
    "\n",
    "class PetNet(nn.Module):\n",
    "\tdef __init__(self, model_name=params['model'], out_features=params['out_features'],\n",
    "\t             inp_channels=params['inp_channels'],\n",
    "\t             pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n",
    "\t\tn_features = self.model.head.in_features\n",
    "\t\tself.model.head = nn.Linear(n_features, 128)\n",
    "\t\tself.fc = nn.Sequential(\n",
    "\t\t\tnn.Linear(128 + num_dense, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, out_features)\n",
    "\t\t)\n",
    "\t\tself.dropout = nn.Dropout(params['dropout'])\n",
    "\n",
    "\tdef forward(self, image, dense):\n",
    "\t\tembeddings = self.model(image)\n",
    "\t\tx = self.dropout(embeddings)\n",
    "\t\tx = torch.cat([x, dense], dim=1)\n",
    "\t\toutput = self.fc(x)\n",
    "\t\treturn output, x\n",
    "\n",
    "\n",
    "def get_test_transforms(DIM=params['im_size']):\n",
    "\treturn albumentations.Compose(\n",
    "\t\t[\n",
    "\t\t\talbumentations.Resize(DIM, DIM),\n",
    "\t\t\talbumentations.Normalize(\n",
    "\t\t\t\tmean=[0.485, 0.456, 0.406],\n",
    "\t\t\t\tstd=[0.229, 0.224, 0.225],\n",
    "\t\t\t),\n",
    "\t\t\tToTensorV2(p=1.0)\n",
    "\t\t]\n",
    "\t)\n",
    "\n",
    "\n",
    "class CuteDataset(Dataset):\n",
    "\tdef __init__(self, images_filepaths, dense_features, targets, transform=None):\n",
    "\t\tself.images_filepaths = images_filepaths\n",
    "\t\tself.dense_features = dense_features\n",
    "\t\tself.targets = targets\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images_filepaths)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage_filepath = self.images_filepaths[idx]\n",
    "\t\timage = cv2.imread(image_filepath)\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\tif self.transform is not None:\n",
    "\t\t\timage = self.transform(image=image)['image']\n",
    "\n",
    "\t\tdense = self.dense_features[idx, :]\n",
    "\t\tlabel = torch.tensor(self.targets[idx]).float()\n",
    "\t\treturn image, dense, label"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2021-10-03T21:23:46.438483Z",
     "iopub.execute_input": "2021-10-03T21:23:46.438789Z",
     "iopub.status.idle": "2021-10-03T21:23:46.531582Z",
     "shell.execute_reply.started": "2021-10-03T21:23:46.438688Z",
     "shell.execute_reply": "2021-10-03T21:23:46.530928Z"
    },
    "_kg_hide-input": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import RAPIDS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import cuml, pickle\n",
    "from cuml.svm import SVR\n",
    "\n",
    "print('RAPIDS version', cuml.__version__, '\\n')\n",
    "\n",
    "LOAD_SVR_FROM_PATH = None\n",
    "\n",
    "df = pd.read_csv('../input/same-old-creating-folds/train_10folds.csv')\n",
    "print('Train shape:', df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:14:56.132253Z",
     "iopub.execute_input": "2021-10-03T22:14:56.132541Z",
     "iopub.status.idle": "2021-10-03T22:14:56.168927Z",
     "shell.execute_reply.started": "2021-10-03T22:14:56.132512Z",
     "shell.execute_reply": "2021-10-03T22:14:56.168246Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Infer Test and OOF\n",
    "In version 1 of this notebook, we extract train embeddings and train RAPIDS SVR heads. (Click version 1 to see this). In later versions and during Kaggle submit, we load these saved RAPIDS SVR fold models and just infer data (without training anything)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "super_final_predictions = []\n",
    "super_final_predictions2 = []\n",
    "super_final_oof_predictions = []\n",
    "super_final_oof_predictions2 = []\n",
    "super_final_oof_true = []\n",
    "\n",
    "for fold_, model_name in zip(range(10), glob.glob(models_dir + '/*.pth')):\n",
    "\tprint('#' * 25)\n",
    "\tprint('### FOLD', fold_ + 1)\n",
    "\tprint('#' * 25)\n",
    "\n",
    "\tmodel = PetNet()\n",
    "\tmodel.load_state_dict(torch.load(model_name))\n",
    "\tmodel = model.to(params[\"device\"])\n",
    "\tmodel.eval()\n",
    "\n",
    "\tdf_test = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n",
    "\ttest_img_paths = [f\"../input/petfinder-pawpularity-score/test/{x}.jpg\" for x in df_test[\"Id\"].values]\n",
    "\n",
    "\tdf_valid = df[df.kfold == fold_].reset_index(drop=True)  #.iloc[:160]\n",
    "\tvalid_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
    "\n",
    "\tdense_features = [\n",
    "\t\t'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "\t\t'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
    "\t]\n",
    "\n",
    "\tname = f\"SVR_fold_{fold_}.pkl\"\n",
    "\tif LOAD_SVR_FROM_PATH is None:\n",
    "\t\t##################\n",
    "\t\t# EXTRACT TRAIN EMBEDDINGS\n",
    "\n",
    "\t\tdf_train = df[df.kfold != fold_].reset_index(drop=True)  #.iloc[:320]\n",
    "\t\ttrain_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_train[\"Id\"].values]\n",
    "\n",
    "\t\ttrain_dataset = CuteDataset(\n",
    "\t\t\timages_filepaths=train_img_paths,\n",
    "\t\t\tdense_features=df_train[dense_features].values,\n",
    "\t\t\ttargets=df_train['Pawpularity'].values / 100.0,\n",
    "\t\t\ttransform=get_test_transforms(),\n",
    "\t\t)\n",
    "\t\ttrain_loader = DataLoader(\n",
    "\t\t\ttrain_dataset, batch_size=params['batch_size'],\n",
    "\t\t\tshuffle=False, num_workers=params['num_workers'],\n",
    "\t\t\tpin_memory=True\n",
    "\t\t)\n",
    "\t\tprint('Extracting train embedding...')\n",
    "\t\ttrain_predictions = None\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor (images, dense, target) in tqdm(train_loader, desc=f'Predicting. '):\n",
    "\t\t\t\timages = images.to(params['device'], non_blocking=True)\n",
    "            \tdense = dense.to(params['device'], non_blocking=True)\n",
    "            \tpredictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n",
    "\n",
    "            \tif temp_preds is None:\n",
    "                \ttemp_preds = predictions\n",
    "\t\t\t\telse:\n",
    "                \ttemp_preds = np.vstack((temp_preds, predictions))\n",
    "\n",
    "\n",
    "\n",
    "\t\tembed = np.array([]).reshape((0, 128 + 12))\n",
    "\t\tfor preds in train_predictions:\n",
    "\t\t\tembed = np.concatenate([embed, preds[:, 1:]], axis=0)\n",
    "\n",
    "\t\t##################\n",
    "\t\t# FIT RAPIDS SVR\n",
    "\t\tprint('Fitting SVR...')\n",
    "\t\tclf = SVR(C=20.0)\n",
    "\t\tclf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n",
    "\n",
    "\t\t##################\n",
    "\t\t# SAVE RAPIDS SVR\n",
    "\t\tpickle.dump(clf, open(name, \"wb\"))\n",
    "\n",
    "\telse:\n",
    "\t\t##################\n",
    "\t\t# LOAD RAPIDS SVR\n",
    "\t\tprint('Loading SVR...', LOAD_SVR_FROM_PATH + name)\n",
    "\t\tclf = pickle.load(open(LOAD_SVR_FROM_PATH + name, \"rb\"))\n",
    "\n",
    "\t##################\n",
    "\t# TEST PREDICTIONS\n",
    "\ttest_dataset = PawpularDataset(\n",
    "\t\timage_paths=test_img_paths,\n",
    "\t\tdense_features=df_test[dense_features].values,\n",
    "\t\ttargets=np.ones(len(test_img_paths)),\n",
    "\t\taugmentations=test_aug,\n",
    "\t)\n",
    "\tprint('Predicting test...')\n",
    "\ttest_predictions = model.predict(test_dataset, batch_size=2 * args.batch_size, n_jobs=-1)\n",
    "\n",
    "\tfinal_test_predictions = []\n",
    "\tembed = np.array([]).reshape((0, 128 + 12))\n",
    "\tfor preds in test_predictions:  #tqdm\n",
    "\t\tfinal_test_predictions.extend(preds[:, :1].ravel().tolist())\n",
    "\t\tembed = np.concatenate([embed, preds[:, 1:]], axis=0)\n",
    "\n",
    "\tfinal_test_predictions = [sigmoid(x) * 100 for x in final_test_predictions]\n",
    "\tfinal_test_predictions2 = clf.predict(embed)\n",
    "\tsuper_final_predictions.append(final_test_predictions)\n",
    "\tsuper_final_predictions2.append(final_test_predictions2)\n",
    "\t##################\n",
    "\n",
    "\t##################\n",
    "\t# OOF PREDICTIONS\n",
    "\tvalid_dataset = PawpularDataset(\n",
    "\t\timage_paths=valid_img_paths,\n",
    "\t\tdense_features=df_valid[dense_features].values,\n",
    "\t\ttargets=df_valid['Pawpularity'].values / 100.0,\n",
    "\t\taugmentations=test_aug,\n",
    "\t)\n",
    "\tprint('Predicting oof...')\n",
    "\tvalid_predictions = model.predict(valid_dataset, batch_size=2 * args.batch_size, n_jobs=-1)\n",
    "\n",
    "\tfinal_oof_predictions = []\n",
    "\tembed = np.array([]).reshape((0, 128 + 12))\n",
    "\tfor preds in valid_predictions:\n",
    "\t\tfinal_oof_predictions.extend(preds[:, :1].ravel().tolist())\n",
    "\t\tembed = np.concatenate([embed, preds[:, 1:]], axis=0)\n",
    "\n",
    "\tfinal_oof_predictions = [sigmoid(x) * 100 for x in final_oof_predictions]\n",
    "\tfinal_oof_predictions2 = clf.predict(embed)\n",
    "\tsuper_final_oof_predictions.append(final_oof_predictions)\n",
    "\tsuper_final_oof_predictions2.append(final_oof_predictions2)\n",
    "\n",
    "\tfinal_oof_true = df_valid['Pawpularity'].values\n",
    "\tsuper_final_oof_true.append(final_oof_true)\n",
    "\t##################\n",
    "\n",
    "\t##################\n",
    "\t# COMPUTE RSME\n",
    "\trsme = np.sqrt(np.mean((super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1])) ** 2.0))\n",
    "\tprint('NN RSME =', rsme, '\\n')\n",
    "\trsme = np.sqrt(np.mean((super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1])) ** 2.0))\n",
    "\tprint('SVR RSME =', rsme, '\\n')\n",
    "\n",
    "\tw = 0.5\n",
    "\toof2 = (1 - w) * np.array(super_final_oof_predictions[-1]) + w * np.array(super_final_oof_predictions2[-1])\n",
    "\trsme = np.sqrt(np.mean((super_final_oof_true[-1] - oof2) ** 2.0))\n",
    "\tprint('Ensemble RSME =', rsme, '\\n')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:30:06.51072Z",
     "iopub.execute_input": "2021-10-03T22:30:06.511064Z",
     "iopub.status.idle": "2021-10-03T22:31:46.891853Z",
     "shell.execute_reply.started": "2021-10-03T22:30:06.51103Z",
     "shell.execute_reply": "2021-10-03T22:31:46.888748Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute CV Score\n",
    "Below we compute the overall CV RSME scores of just the NN head, just the SVR head, and an ensemble of 50% NN and 50% SVR heads. Then we plot all ensemble weights to find the optimal weights for NN head and SVR heads."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "true = np.hstack(super_final_oof_true)\n",
    "\n",
    "oof = np.hstack(super_final_oof_predictions)\n",
    "rsme = np.sqrt(np.mean((oof - true) ** 2.0))\n",
    "print('Overall CV NN head RSME =', rsme)\n",
    "\n",
    "oof2 = np.hstack(super_final_oof_predictions2)\n",
    "rsme = np.sqrt(np.mean((oof2 - true) ** 2.0))\n",
    "print('Overall CV SVR head RSME =', rsme)\n",
    "\n",
    "oof3 = (1 - w) * oof + w * oof2\n",
    "rsme = np.sqrt(np.mean((oof3 - true) ** 2.0))\n",
    "print('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =', rsme)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:41:10.879351Z",
     "iopub.execute_input": "2021-10-03T22:41:10.880014Z",
     "iopub.status.idle": "2021-10-03T22:41:10.89265Z",
     "shell.execute_reply.started": "2021-10-03T22:41:10.87998Z",
     "shell.execute_reply": "2021-10-03T22:41:10.891905Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "score = []\n",
    "for ww in np.arange(0, 1.05, 0.05):\n",
    "\toof3 = (1 - ww) * oof + ww * oof2\n",
    "\trsme = np.sqrt(np.mean((oof3 - true) ** 2.0))\n",
    "\t#print(f'{ww:0.2} CV Ensemble RSME =',rsme)\n",
    "\tscore.append(rsme)\n",
    "best_w = np.argmin(score) * 0.05\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(np.arange(21) / 20.0, score, '-o')\n",
    "plt.plot([best_w], np.min(score), 'o', color='black', markersize=15)\n",
    "plt.title(f'Best Overall CV RSME={np.min(score):.4} with SVR Ensemble Weight={best_w:.2}', size=16)\n",
    "plt.ylabel('Overall Ensemble RSME', size=14)\n",
    "plt.xlabel('SVR Weight', size=14)\n",
    "plt.show()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2021-10-03T22:55:02.912359Z",
     "iopub.execute_input": "2021-10-03T22:55:02.91262Z",
     "iopub.status.idle": "2021-10-03T22:55:03.127998Z",
     "shell.execute_reply.started": "2021-10-03T22:55:02.912593Z",
     "shell.execute_reply": "2021-10-03T22:55:03.12713Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Submission CSV\n",
    "We make a submission csv using an ensemble of both heads. We use the optimal ensemble weights that we discovered above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\n",
    "super_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\n",
    "df_test[\"Pawpularity\"] = (1 - best_w) * super_final_predictions + best_w * super_final_predictions2\n",
    "df_test = df_test[[\"Id\", \"Pawpularity\"]]\n",
    "df_test.to_csv(\"submission.csv\", index=False)\n",
    "df_test.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:42:02.159016Z",
     "iopub.execute_input": "2021-10-03T22:42:02.159579Z",
     "iopub.status.idle": "2021-10-03T22:42:02.180502Z",
     "shell.execute_reply.started": "2021-10-03T22:42:02.159542Z",
     "shell.execute_reply": "2021-10-03T22:42:02.179627Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}