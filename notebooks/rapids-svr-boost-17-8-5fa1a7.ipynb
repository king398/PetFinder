{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Boost CV LB +0.2 with RAPIDS SVR Head\nIn this notebook, we demonstrate how to add a RAPIDS SVR second head to an already trained CNN or Image Transformer with first head. This boosts CV LB by +0.2! This trick was used in CommonLit Comp [here][3] to boost our team into Gold Medal!\n\nWe begin with [Abhishek's][2] public notebook [here][1] which already contains a fully trained Image Transformer model (with NN head). Now in this notebook, we extract the image embeddings (from the trained fold models) and train additional RAPIDS SVR heads for each fold. The original NN head achieves overall CV RSME 18.0 and the new RAPIDS SVR head achieves overall CV RSME 18.0. Both heads are very diverse because the NN head uses Classification (BCE) loss and the SVR head uses Regression loss. During inference, we predict with both heads. When we average both heads' predictions, we achieve overall CV RSME 17.8!\n\nThe technique illustrated here can be applied to any trained image (or NLP) model for CV LB boost! In the first version of this notebook, we train the SVR heads and save the fold models. Then, in later notebook versions and during Kaggle submission, we load the saved SVR models (from this notebook's version 1 which was made into a Kaggle dataset).\n\n[1]: https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n[2]: https://www.kaggle.com/abhishek\n[3]: https://www.kaggle.com/c/commonlitreadabilityprize/discussion/260800",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Load Libraries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# based on the post here: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\nsys.path.append(\"../input/timmmaster/\")\n\nimport tez\nimport albumentations\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport timm\nimport torch.nn as nn\nfrom sklearn import metrics\nimport torch\nfrom tez.callbacks import EarlyStopping\nfrom tqdm import tqdm\nimport math\n\nclass args:\n    batch_size = 16\n    image_size = 384\n    \ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))",
   "metadata": {
    "_kg_hide-input": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Define Swim Model and Swim Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "params = {\n    'model': 'swin_large_patch4_window12_384_in22k',\n    'dense_features': features,\n    'pretrained': False,\n    'inp_channels': 3,\n    'im_size': 384,\n    'device': device,\n    'batch_size': 8,\n    'num_workers' : 2,\n    'out_features': 1,\n    'debug': False\n}\nclass PetNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'],\n                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, 128)\n        self.fc = nn.Sequential(\n            nn.Linear(128 + num_dense, 64),\n            nn.ReLU(),\n            nn.Linear(64, out_features)\n        )\n        self.dropout = nn.Dropout(params['dropout'])\n    \n    def forward(self, image, dense):\n        embeddings = self.model(image)\n        x = self.dropout(embeddings)\n        x = torch.cat([x, dense], dim=1)\n        output = self.fc(x)\n        return output,x\n\ndef get_test_transforms(DIM = params['im_size']):\n    return albumentations.Compose(\n        [\n          albumentations.Resize(DIM,DIM),\n          albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n          ),\n          ToTensorV2(p=1.0)\n        ]\n    )\nclass CuteDataset(Dataset):\n    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        dense = self.dense_features[idx, :]\n        label = torch.tensor(self.targets[idx]).float()\n        return image, dense, label",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2021-10-03T21:23:46.438483Z",
     "iopub.execute_input": "2021-10-03T21:23:46.438789Z",
     "iopub.status.idle": "2021-10-03T21:23:46.531582Z",
     "shell.execute_reply.started": "2021-10-03T21:23:46.438688Z",
     "shell.execute_reply": "2021-10-03T21:23:46.530928Z"
    },
    "_kg_hide-input": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Import RAPIDS",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import cuml, pickle\nfrom cuml.svm import SVR\nprint('RAPIDS version',cuml.__version__,'\\n')\n\nLOAD_SVR_FROM_PATH = None\n\ndf = pd.read_csv('../input/same-old-creating-folds/train_10folds.csv')\nprint('Train shape:', df.shape )\ndf.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:14:56.132253Z",
     "iopub.execute_input": "2021-10-03T22:14:56.132541Z",
     "iopub.status.idle": "2021-10-03T22:14:56.168927Z",
     "shell.execute_reply.started": "2021-10-03T22:14:56.132512Z",
     "shell.execute_reply": "2021-10-03T22:14:56.168246Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Infer Test and OOF\nIn version 1 of this notebook, we extract train embeddings and train RAPIDS SVR heads. (Click version 1 to see this). In later versions and during Kaggle submit, we load these saved RAPIDS SVR fold models and just infer data (without training anything).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "super_final_predictions = []\nsuper_final_predictions2 = []\nsuper_final_oof_predictions = []\nsuper_final_oof_predictions2 = []\nsuper_final_oof_true = []\n\nfor fold_ in range(10):\n    print('#'*25)\n    print('### FOLD',fold_+1)\n    print('#'*25)\n    \n    model = PawpularModel(model_name=\"swin_large_patch4_window12_384\")\n    model.load(f\"../input/paw-models/model_f{fold_}.bin\", device=\"cuda\", weights_only=True)\n\n    df_test = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n    test_img_paths = [f\"../input/petfinder-pawpularity-score/test/{x}.jpg\" for x in df_test[\"Id\"].values]\n        \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)#.iloc[:160]\n    valid_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_valid[\"Id\"].values]\n\n    dense_features = [\n        'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n        'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n    ]\n    \n    name = f\"SVR_fold_{fold_}.pkl\" \n    if LOAD_SVR_FROM_PATH is None:\n        ##################\n        # EXTRACT TRAIN EMBEDDINGS\n        \n        df_train = df[df.kfold != fold_].reset_index(drop=True)#.iloc[:320]\n        train_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_train[\"Id\"].values]\n        \n        train_dataset = PawpularDataset(\n            image_paths=train_img_paths,\n            dense_features=df_train[dense_features].values,\n            targets=df_train['Pawpularity'].values/100.0,\n            augmentations=test_aug,\n        )\n        print('Extracting train embedding...')\n        train_predictions = model.predict(train_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n    \n        embed = np.array([]).reshape((0,128+12))\n        for preds in train_predictions:\n            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n        \n        ##################\n        # FIT RAPIDS SVR\n        print('Fitting SVR...')\n        clf = SVR(C=20.0)\n        clf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n    \n        ##################\n        # SAVE RAPIDS SVR \n        pickle.dump(clf, open(name, \"wb\"))\n        \n    else:\n        ##################\n        # LOAD RAPIDS SVR \n        print('Loading SVR...',LOAD_SVR_FROM_PATH+name)\n        clf = pickle.load(open(LOAD_SVR_FROM_PATH+name, \"rb\"))\n\n    ##################\n    # TEST PREDICTIONS\n    test_dataset = PawpularDataset(\n        image_paths=test_img_paths,\n        dense_features=df_test[dense_features].values,\n        targets=np.ones(len(test_img_paths)),\n        augmentations=test_aug,\n    )\n    print('Predicting test...')\n    test_predictions = model.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_test_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in test_predictions: #tqdm\n        final_test_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_test_predictions = [sigmoid(x) * 100 for x in final_test_predictions]\n    final_test_predictions2 = clf.predict(embed)\n    super_final_predictions.append(final_test_predictions)\n    super_final_predictions2.append(final_test_predictions2)\n    ##################\n    \n    ##################\n    # OOF PREDICTIONS\n    valid_dataset = PawpularDataset(\n        image_paths=valid_img_paths,\n        dense_features=df_valid[dense_features].values,\n        targets=df_valid['Pawpularity'].values/100.0,\n        augmentations=test_aug,\n    )\n    print('Predicting oof...')\n    valid_predictions = model.predict(valid_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_oof_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in valid_predictions:\n        final_oof_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_oof_predictions = [sigmoid(x) * 100 for x in final_oof_predictions]\n    final_oof_predictions2 = clf.predict(embed)    \n    super_final_oof_predictions.append(final_oof_predictions)\n    super_final_oof_predictions2.append(final_oof_predictions2)\n    \n    final_oof_true = df_valid['Pawpularity'].values\n    super_final_oof_true.append(final_oof_true)\n    ##################\n    \n    ##################\n    # COMPUTE RSME\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1]))**2.0 ) )\n    print('NN RSME =',rsme,'\\n')\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1]))**2.0 ) )\n    print('SVR RSME =',rsme,'\\n')\n    \n    w = 0.5\n    oof2 = (1-w)*np.array(super_final_oof_predictions[-1]) + w*np.array(super_final_oof_predictions2[-1])\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - oof2)**2.0 ) )\n    print('Ensemble RSME =',rsme,'\\n')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:30:06.51072Z",
     "iopub.execute_input": "2021-10-03T22:30:06.511064Z",
     "iopub.status.idle": "2021-10-03T22:31:46.891853Z",
     "shell.execute_reply.started": "2021-10-03T22:30:06.51103Z",
     "shell.execute_reply": "2021-10-03T22:31:46.888748Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Compute CV Score\nBelow we compute the overall CV RSME scores of just the NN head, just the SVR head, and an ensemble of 50% NN and 50% SVR heads. Then we plot all ensemble weights to find the optimal weights for NN head and SVR heads.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "true = np.hstack(super_final_oof_true)\n\noof = np.hstack(super_final_oof_predictions)\nrsme = np.sqrt( np.mean( (oof - true)**2.0 ))\nprint('Overall CV NN head RSME =',rsme)\n\noof2 = np.hstack(super_final_oof_predictions2)\nrsme = np.sqrt( np.mean( (oof2 - true)**2.0 ))\nprint('Overall CV SVR head RSME =',rsme)\n\noof3 = (1-w)*oof + w*oof2\nrsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\nprint('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =',rsme)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:41:10.879351Z",
     "iopub.execute_input": "2021-10-03T22:41:10.880014Z",
     "iopub.status.idle": "2021-10-03T22:41:10.89265Z",
     "shell.execute_reply.started": "2021-10-03T22:41:10.87998Z",
     "shell.execute_reply": "2021-10-03T22:41:10.891905Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\nscore = []\nfor ww in np.arange(0,1.05,0.05):\n    oof3 = (1-ww)*oof + ww*oof2\n    rsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\n    #print(f'{ww:0.2} CV Ensemble RSME =',rsme)\n    score.append(rsme)\nbest_w = np.argmin(score)*0.05\n\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(21)/20.0,score,'-o')\nplt.plot([best_w],np.min(score),'o',color='black',markersize=15)\nplt.title(f'Best Overall CV RSME={np.min(score):.4} with SVR Ensemble Weight={best_w:.2}',size=16)\nplt.ylabel('Overall Ensemble RSME',size=14)\nplt.xlabel('SVR Weight',size=14)\nplt.show()",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2021-10-03T22:55:02.912359Z",
     "iopub.execute_input": "2021-10-03T22:55:02.91262Z",
     "iopub.status.idle": "2021-10-03T22:55:03.127998Z",
     "shell.execute_reply.started": "2021-10-03T22:55:02.912593Z",
     "shell.execute_reply": "2021-10-03T22:55:03.12713Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Make Submission CSV\nWe make a submission csv using an ensemble of both heads. We use the optimal ensemble weights that we discovered above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\nsuper_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\ndf_test[\"Pawpularity\"] = (1-best_w)*super_final_predictions + best_w*super_final_predictions2\ndf_test = df_test[[\"Id\", \"Pawpularity\"]]\ndf_test.to_csv(\"submission.csv\", index=False)\ndf_test.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-03T22:42:02.159016Z",
     "iopub.execute_input": "2021-10-03T22:42:02.159579Z",
     "iopub.status.idle": "2021-10-03T22:42:02.180502Z",
     "shell.execute_reply.started": "2021-10-03T22:42:02.159542Z",
     "shell.execute_reply": "2021-10-03T22:42:02.179627Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}