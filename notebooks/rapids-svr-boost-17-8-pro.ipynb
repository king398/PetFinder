{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Boost CV LB +0.2 with RAPIDS SVR Head\n",
    "In this notebook, we demonstrate how to add a RAPIDS SVR second head to an already trained CNN or Image Transformer with first head. This boosts CV LB by +0.2! This trick was used in CommonLit Comp [here][3] to boost our team into Gold Medal!\n",
    "\n",
    "We begin with [Abhishek's][2] public notebook [here][1] which already contains a fully trained Image Transformer model (with NN head). Now in this notebook, we extract the image embeddings (from the trained fold models) and train additional RAPIDS SVR heads for each fold. The original NN head achieves overall CV RSME 18.0 and the new RAPIDS SVR head achieves overall CV RSME 18.0. Both heads are very diverse because the NN head uses Classification (BCE) loss and the SVR head uses Regression loss. During inference, we predict with both heads. When we average both heads' predictions, we achieve overall CV RSME 17.8!\n",
    "\n",
    "The technique illustrated here can be applied to any trained image (or NLP) model for CV LB boost! In the first version of this notebook, we train the SVR heads and save the fold models. Then, in later notebook versions and during Kaggle submission, we load the saved SVR models (from this notebook's version 1 which was made into a Kaggle dataset).\n",
    "\n",
    "[1]: https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n",
    "[2]: https://www.kaggle.com/abhishek\n",
    "[3]: https://www.kaggle.com/c/commonlitreadabilityprize/discussion/260800"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.009855,
     "end_time": "2021-10-05T01:29:50.720532",
     "exception": false,
     "start_time": "2021-10-05T01:29:50.710677",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to Add RAPIDS SVR Head\n",
    "There are 3 steps to building a double headed model. The first step is to train your Image NN backbone and head. This was done by Abhishek in his notebook [here][1] and achieves CV RSME 18.0. The next step is to train our RAPIDS SVR head with extracted embeddings from frozen Image NN backbone. This is done in version 1 of notebook you are reading [here][2] and achieves CV RSME 18.0. Lastly, we infer with both heads and average the predictions. This is done in the notebook you are reading and achieves CV RSME 17.8!\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st1.png)\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st2.png)\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st3.png)\n",
    "\n",
    "[1]: https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n",
    "[2]: https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8?scriptVersionId=76282086"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Libraries"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.008276,
     "end_time": "2021-10-05T01:29:50.73785",
     "exception": false,
     "start_time": "2021-10-05T01:29:50.729574",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# based on the post here: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../input/tez-lib/\")\n",
    "sys.path.append(\"../input/timmmaster/\")\n",
    "\n",
    "import tez\n",
    "import albumentations\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from tez.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "class args:\n",
    "\tbatch_size = 16\n",
    "\timage_size = 384\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "\treturn 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "params = {\n",
    "\t'model': 'swin_large_patch4_window12_384_in22k',\n",
    "\t'pretrained': True,\n",
    "\t'inp_channels': 3,\n",
    "\t'im_size': 384,\n",
    "\n",
    "\t'lr': 1e-5,\n",
    "\t'weight_decay': 1e-6,\n",
    "\t'batch_size': 8,\n",
    "\t'num_workers': 2,\n",
    "\t'epochs': 10,\n",
    "\t'out_features': 1,\n",
    "\t'dropout': 0.2,\n",
    "\t'num_fold': 10,\n",
    "\t'mixup': False,\n",
    "\t'mixup_alpha': 1.0,\n",
    "\t'scheduler_name': 'CosineAnnealingWarmRestarts',\n",
    "\t'T_0': 5,\n",
    "\t'T_max': 5,\n",
    "\t'T_mult': 1,\n",
    "\t'min_lr': 1e-7,\n",
    "\t'max_lr': 1e-4\n",
    "}\n"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 8.607495,
     "end_time": "2021-10-05T01:29:59.353831",
     "exception": false,
     "start_time": "2021-10-05T01:29:50.746336",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Swim Model and Swim Dataset"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.008453,
     "end_time": "2021-10-05T01:29:59.371878",
     "exception": false,
     "start_time": "2021-10-05T01:29:59.363425",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PawpularDataset:\n",
    "\tdef __init__(self, image_paths, dense_features, targets, augmentations):\n",
    "\t\tself.image_paths = image_paths\n",
    "\t\tself.dense_features = dense_features\n",
    "\t\tself.targets = targets\n",
    "\t\tself.augmentations = augmentations\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.image_paths)\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\timage = cv2.imread(self.image_paths[item])\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t\tif self.augmentations is not None:\n",
    "\t\t\taugmented = self.augmentations(image=image)\n",
    "\t\t\timage = augmented[\"image\"]\n",
    "\n",
    "\t\timage = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "\t\tfeatures = self.dense_features[item, :]\n",
    "\t\ttargets = self.targets[item]\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"image\": torch.tensor(image, dtype=torch.float),\n",
    "\t\t\t\"features\": torch.tensor(features, dtype=torch.float),\n",
    "\t\t\t\"targets\": torch.tensor(targets, dtype=torch.float),\n",
    "\t\t}\n",
    "\n",
    "\n",
    "class PetNet(nn.Module):\n",
    "\tdef __init__(self, model_name=params['model'], out_features=params['out_features'],\n",
    "\t             inp_channels=params['inp_channels'],\n",
    "\t             pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n",
    "\t\tn_features = self.model.head.in_features\n",
    "\t\tself.model.head = nn.Linear(n_features, 128)\n",
    "\t\tself.fc = nn.Sequential(\n",
    "\t\t\tnn.Linear(128 + num_dense, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, out_features)\n",
    "\t\t)\n",
    "\t\tself.dropout = nn.Dropout(params['dropout'])\n",
    "\n",
    "\tdef forward(self, image, dense):\n",
    "\t\tembeddings = self.model(image)\n",
    "\t\tx = self.dropout(embeddings)\n",
    "\t\tx = torch.cat([x, dense], dim=1)\n",
    "\t\toutput = self.fc(x)\n",
    "\t\treturn output, 0, {}\n",
    "\n",
    "\n",
    "test_aug = albumentations.Compose(\n",
    "\t[\n",
    "\t\talbumentations.Resize(args.image_size, args.image_size, p=1),\n",
    "\n",
    "\t\talbumentations.Normalize(\n",
    "\t\t\tmean=[0.485, 0.456, 0.406],\n",
    "\t\t\tstd=[0.229, 0.224, 0.225],\n",
    "\t\t\tmax_pixel_value=255.0,\n",
    "\t\t\tp=1.0,\n",
    "\t\t),\n",
    "\t],\n",
    "\tp=1.0,\n",
    ")"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.028308,
     "end_time": "2021-10-05T01:29:59.409017",
     "exception": false,
     "start_time": "2021-10-05T01:29:59.380709",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import RAPIDS"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.00919,
     "end_time": "2021-10-05T01:29:59.427243",
     "exception": false,
     "start_time": "2021-10-05T01:29:59.418053",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import cuml, pickle\n",
    "from cuml.svm import SVR\n",
    "\n",
    "print('RAPIDS version', cuml.__version__, '\\n')\n",
    "\n",
    "LOAD_SVR_FROM_PATH = '../input/svr-models-10-folds/'\n",
    "\n",
    "df = pd.read_csv('../input/same-old-creating-folds/train_10folds.csv')\n",
    "print('Train shape:', df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "papermill": {
     "duration": 3.453807,
     "end_time": "2021-10-05T01:30:02.890319",
     "exception": false,
     "start_time": "2021-10-05T01:29:59.436512",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Infer Test and OOF\n",
    "In version 1 of this notebook, we extract train embeddings and train RAPIDS SVR heads. (Click version 1 to see this). In later versions and during Kaggle submit, we load these saved RAPIDS SVR fold models and just infer data (without training anything)."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.00976,
     "end_time": "2021-10-05T01:30:02.910957",
     "exception": false,
     "start_time": "2021-10-05T01:30:02.901197",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "super_final_predictions = []\n",
    "super_final_predictions2 = []\n",
    "super_final_oof_predictions = []\n",
    "super_final_oof_predictions2 = []\n",
    "super_final_oof_true = []\n",
    "\n",
    "for fold_, model_name in zip(range(10), glob.glob(\"../input/swin-transformenrs-pet-net/*.pth\")):\n",
    "\tprint('#' * 25)\n",
    "\tprint('### FOLD', fold_ + 1)\n",
    "\tprint('#' * 25)\n",
    "\n",
    "\tmodel = PetNet(model_name=\"swin_large_patch4_window12_384\")\n",
    "\tdog_state = torch.load(model_name);\n",
    "\tmodel.load_state_dict(dog_state)\n",
    "\n",
    "\tdf_test = pd.read_csv(\"../input/petfinder-pawpularity-score/test.csv\")\n",
    "\ttest_img_paths = [f\"../input/petfinder-pawpularity-score/test/{x}.jpg\" for x in df_test[\"Id\"].values]\n",
    "\n",
    "\tdf_valid = df[df.kfold == fold_].reset_index(drop=True)  #.iloc[:160]\n",
    "\tvalid_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
    "\n",
    "\tdense_features = [\n",
    "\t\t'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "\t\t'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
    "\t]\n",
    "\n",
    "\tname = f\"SVR_fold_{fold_}.pkl\"\n",
    "\tif LOAD_SVR_FROM_PATH is None:\n",
    "\t\t##################\n",
    "\t\t# EXTRACT TRAIN EMBEDDINGS\n",
    "\n",
    "\t\tdf_train = df[df.kfold != fold_].reset_index(drop=True)  #.iloc[:320]\n",
    "\t\ttrain_img_paths = [f\"../input/petfinder-pawpularity-score/train/{x}.jpg\" for x in df_train[\"Id\"].values]\n",
    "\n",
    "\t\ttrain_dataset = PawpularDataset(\n",
    "\t\t\timage_paths=train_img_paths,\n",
    "\t\t\tdense_features=df_train[dense_features].values,\n",
    "\t\t\ttargets=df_train['Pawpularity'].values / 100.0,\n",
    "\t\t\taugmentations=test_aug,\n",
    "\t\t)\n",
    "\t\tprint('Extracting train embedding...')\n",
    "\t\ttrain_predictions = []\n",
    "\t\tfor images,dense in enumerate(PawpularDataset):\n",
    "\t\t\twith torch.no_grad()\n",
    "\n",
    "\n",
    "\t\tembed = np.array([]).reshape((0, 128 + 12))\n",
    "\t\tfor preds in train_predictions:\n",
    "\t\t\tembed = np.concatenate([embed, preds[:, 1:]], axis=0)\n",
    "\n",
    "\t\t##################\n",
    "\t\t# FIT RAPIDS SVR\n",
    "\t\tprint('Fitting SVR...')\n",
    "\t\tclf = SVR(C=20.0)\n",
    "\t\tclf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n",
    "\n",
    "\t\t##################\n",
    "\t\t# SAVE RAPIDS SVR\n",
    "\t\tpickle.dump(clf, open(name, \"wb\"))\n",
    "\n",
    "\telse:\n",
    "\t\t##################\n",
    "\t\t# LOAD RAPIDS SVR\n",
    "\t\tprint('Loading SVR...', LOAD_SVR_FROM_PATH + name)\n",
    "\t\tclf = pickle.load(open(LOAD_SVR_FROM_PATH + name, \"rb\"))\n",
    "\n",
    "\t##################\n",
    "\t# TEST PREDICTIONS\n",
    "\ttest_dataset = PawpularDataset(\n",
    "\t\timage_paths=test_img_paths,\n",
    "\t\tdense_features=df_test[dense_features].values,\n",
    "\t\ttargets=np.ones(len(test_img_paths)),\n",
    "\t\taugmentations=test_aug,\n",
    "\t)\n",
    "\tprint('Predicting test...')\n",
    "\ttest_predictions = model.predict(test_dataset, batch_size=2 * args.batch_size, n_jobs=-1)\n",
    "\n",
    "\tfinal_test_predictions = []\n",
    "\tembed = np.array([]).reshape((0, 128 + 12))\n",
    "\tfor preds in test_predictions:\n",
    "\t\tfinal_test_predictions.extend(preds[:, :1].ravel().tolist())\n",
    "\t\tembed = np.column_stack([embed, preds[:, 1:]], axis=0)\n",
    "\tprint(\"embed shape:\" + str(embed.shape))\n",
    "\n",
    "\tfinal_test_predictions = [sigmoid(x) * 100 for x in final_test_predictions]\n",
    "\tfinal_test_predictions2 = clf.predict(embed)\n",
    "\tsuper_final_predictions.append(final_test_predictions)\n",
    "\tsuper_final_predictions2.append(final_test_predictions2)\n",
    "\t##################\n",
    "\n",
    "\t##################\n",
    "\t# OOF PREDICTIONS\n",
    "\tvalid_dataset = PawpularDataset(\n",
    "\t\timage_paths=valid_img_paths,\n",
    "\t\tdense_features=df_valid[dense_features].values,\n",
    "\t\ttargets=df_valid['Pawpularity'].values / 100.0,\n",
    "\t\taugmentations=test_aug,\n",
    "\t)\n",
    "\tprint('Predicting oof...')\n",
    "\tvalid_predictions = model.predict(valid_dataset, batch_size=2 * args.batch_size, n_jobs=-1)\n",
    "\n",
    "\tfinal_oof_predictions = []\n",
    "\tembed = np.array([]).reshape((0, 128 + 12))\n",
    "\tfor preds in valid_predictions:\n",
    "\t\tfinal_oof_predictions.extend(preds[:, :1].ravel().tolist())\n",
    "\t\tembed = np.concatenate([embed, preds[:, 1:]], axis=0)\n",
    "\n",
    "\tfinal_oof_predictions = [sigmoid(x) * 100 for x in final_oof_predictions]\n",
    "\tfinal_oof_predictions2 = clf.predict(embed)\n",
    "\tsuper_final_oof_predictions.append(final_oof_predictions)\n",
    "\tsuper_final_oof_predictions2.append(final_oof_predictions2)\n",
    "\n",
    "\tfinal_oof_true = df_valid['Pawpularity'].values\n",
    "\tsuper_final_oof_true.append(final_oof_true)\n",
    "\t##################\n",
    "\n",
    "\t##################\n",
    "\t# COMPUTE RSME\n",
    "\trsme = np.sqrt(np.mean((super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1])) ** 2.0))\n",
    "\tprint('NN RSME =', rsme, '\\n')\n",
    "\trsme = np.sqrt(np.mean((super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1])) ** 2.0))\n",
    "\tprint('SVR RSME =', rsme, '\\n')\n",
    "\n",
    "\tw = 0.5\n",
    "\toof2 = (1 - w) * np.array(super_final_oof_predictions[-1]) + w * np.array(super_final_oof_predictions2[-1])\n",
    "\trsme = np.sqrt(np.mean((super_final_oof_true[-1] - oof2) ** 2.0))\n",
    "\tprint('Ensemble RSME =', rsme, '\\n')"
   ],
   "metadata": {
    "papermill": {
     "duration": 569.218368,
     "end_time": "2021-10-05T01:39:32.138933",
     "exception": false,
     "start_time": "2021-10-05T01:30:02.920565",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute CV Score\n",
    "Below we compute the overall CV RSME scores of just the NN head, just the SVR head, and an ensemble of 50% NN and 50% SVR heads. Then we plot all ensemble weights to find the optimal weights for NN head and SVR heads."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.194813,
     "end_time": "2021-10-05T01:39:32.538415",
     "exception": false,
     "start_time": "2021-10-05T01:39:32.343602",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "true = np.hstack(super_final_oof_true)\n",
    "\n",
    "oof = np.hstack(super_final_oof_predictions)\n",
    "rsme = np.sqrt(np.mean((oof - true) ** 2.0))\n",
    "print('Overall CV NN head RSME =', rsme)\n",
    "\n",
    "oof2 = np.hstack(super_final_oof_predictions2)\n",
    "rsme = np.sqrt(np.mean((oof2 - true) ** 2.0))\n",
    "print('Overall CV SVR head RSME =', rsme)\n",
    "\n",
    "oof3 = (1 - w) * oof + w * oof2\n",
    "rsme = np.sqrt(np.mean((oof3 - true) ** 2.0))\n",
    "print('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =', rsme)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.234202,
     "end_time": "2021-10-05T01:39:32.976703",
     "exception": false,
     "start_time": "2021-10-05T01:39:32.742501",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "score = []\n",
    "for ww in np.arange(0, 1.05, 0.05):\n",
    "\toof3 = (1 - ww) * oof + ww * oof2\n",
    "\trsme = np.sqrt(np.mean((oof3 - true) ** 2.0))\n",
    "\t#print(f'{ww:0.2} CV Ensemble RSME =',rsme)\n",
    "\tscore.append(rsme)\n",
    "best_w = np.argmin(score) * 0.05\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(np.arange(21) / 20.0, score, '-o')\n",
    "plt.plot([best_w], np.min(score), 'o', color='black', markersize=15)\n",
    "plt.title(f'Best Overall CV RSME={np.min(score):.4} with SVR Ensemble Weight={best_w:.2}', size=16)\n",
    "plt.ylabel('Overall Ensemble RSME', size=14)\n",
    "plt.xlabel('SVR Weight', size=14)\n",
    "plt.show()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.473663,
     "end_time": "2021-10-05T01:39:33.662871",
     "exception": false,
     "start_time": "2021-10-05T01:39:33.189208",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trust CV or LB?\n",
    "Above we see that using 50% NN head and 50% SVR head achieves the best overall CV score. However our RAPIDS SVR head isn't helping public LB much. We also notice that our RAPIDS SVR head helped folds `1, 2, 4, 5, 7, 8, 9, 10` but did not help folds `3, 6`. So is public test data just a \"bad fold\"? Will our RAPIDS SVR head help private LB? Below we force the weight of SVR head to be 10% in order to achieve a slight public LB boost. But maybe for final submission, we should use 50%??"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# FORCE SVR WEIGHT TO LOWER VALUE TO HELP PUBLIC LB\n",
    "best_w = 0.2"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Submission CSV\n",
    "We make a submission csv using an ensemble of both heads. We use the optimal ensemble weights that we discovered above."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.209965,
     "end_time": "2021-10-05T01:39:34.083427",
     "exception": false,
     "start_time": "2021-10-05T01:39:33.873462",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\n",
    "super_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\n",
    "df_test[\"Pawpularity\"] = (1 - best_w) * super_final_predictions + best_w * super_final_predictions2\n",
    "df_test = df_test[[\"Id\", \"Pawpularity\"]]\n",
    "df_test.to_csv(\"submission.csv\", index=False)\n",
    "df_test.head()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.228278,
     "end_time": "2021-10-05T01:39:34.518785",
     "exception": false,
     "start_time": "2021-10-05T01:39:34.290507",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}